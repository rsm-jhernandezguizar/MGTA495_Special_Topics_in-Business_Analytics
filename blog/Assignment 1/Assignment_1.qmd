---
title: "A Replication of Karlan and List (2007)"
description: "Assignment 1"
image: /images/assignment1.jpg #updated image to be that of fundraising
date: 2025-04-23
author: Juan Hernández Guizar
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---

## Introduction

Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the _American Economic Review_ in 2007. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2).

In their study, Karlan and List discovered that announcing a matching grant significantly boosted both the probability and size of contributions, confirming that even simple price incentives can nudge donors into action. Intriguingly, however, larger match ratios (such as a $3:$1 match) did not outperform smaller ones (like $1:$1), suggesting that bigger “discounts” on giving may not always translate to bigger impacts. They also found the local political environment influenced donor responsiveness: individuals in conservative “red” states were more swayed by the matching offer than those in liberal “blue” states. This highlights that factors like community norms and trust can be just as critical as the financial structure of a fundraising campaign.

This project seeks to replicate their results.


## Data

### Description

To replicate Karlan and List’s findings, I first loaded their dataset and generated preliminary plots to get a feel for its contents. After displaying the first few rows to confirm the structure, I computed donation rates by treatment group and then prepared side-by-side visuals—bar plots for participation and histograms of donation amounts—to highlight the core outcome measures. These initial checks ensure that the data aligns with the original study’s composition before we proceed with more in-depth statistical testing and analysis.

Detailed explanation of all the variables captured

:::: {.callout-note collapse="true"}
### Variable Definitions

| Variable             | Description                                                         |
|----------------------|---------------------------------------------------------------------|
| `treatment`          | Treatment                                                           |
| `control`            | Control                                                             |
| `ratio`              | Match ratio                                                         |
| `ratio2`             | 2:1 match ratio                                                     |
| `ratio3`             | 3:1 match ratio                                                     |
| `size`               | Match threshold                                                     |
| `size25`             | \$25,000 match threshold                                            |
| `size50`             | \$50,000 match threshold                                            |
| `size100`            | \$100,000 match threshold                                           |
| `sizeno`             | Unstated match threshold                                            |
| `ask`                | Suggested donation amount                                           |
| `askd1`              | Suggested donation was highest previous contribution                |
| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |
| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |
| `ask1`               | Highest previous contribution (for suggestion)                      |
| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |
| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |
| `amount`             | Dollars given                                                       |
| `gave`               | Gave anything                                                       |
| `amountchange`       | Change in amount given                                              |
| `hpa`                | Highest previous contribution                                       |
| `ltmedmra`           | Small prior donor: last gift was less than median \$35              |
| `freq`               | Number of prior donations                                           |
| `years`              | Number of years since initial donation                              |
| `year5`              | At least 5 years since initial donation                             |
| `mrm2`               | Number of months since last donation                                |
| `dormant`            | Already donated in 2005                                             |
| `female`             | Female                                                              |
| `couple`             | Couple                                                              |
| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |
| `nonlit`             | Nonlitigation                                                       |
| `cases`              | Court cases from state in 2004-5 in which organization was involved |
| `statecnt`           | Percent of sample from state                                        |
| `stateresponse`      | Proportion of sample from the state who gave                        |
| `stateresponset`     | Proportion of treated sample from the state who gave                |
| `stateresponsec`     | Proportion of control sample from the state who gave                |
| `stateresponsetminc` | stateresponset - stateresponsec                                     |
| `perbush`            | State vote share for Bush                                           |
| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |
| `red0`               | Red state                                                           |
| `blue0`              | Blue state                                                          |
| `redcty`             | Red county                                                          |
| `bluecty`            | Blue county                                                         |
| `pwhite`             | Proportion white within zip code                                    |
| `pblack`             | Proportion black within zip code                                    |
| `page18_39`          | Proportion age 18-39 within zip code                                |
| `ave_hh_sz`          | Average household size within zip code                              |
| `median_hhincome`    | Median household income within zip code                             |
| `powner`             | Proportion house owner within zip code                              |
| `psch_atlstba`       | Proportion who finished college within zip code                     |
| `pop_propurban`      | Proportion of population urban within zip code                      |

::::

Preview of results from study

```{python}
# | echo: false
import pandas as pd

# Correct path to file stored in the local _files folder
file_path = "Assignment_1_data/karlan_list_2007.dta"

# Load the .dta file using the correct relative path
karlan_list_2007_pretty = pd.read_stata(file_path)
karlan_list_2007_int_codes = pd.read_stata(file_path)

# Preview
karlan_list_2007_pretty.head()
```

Bar plot – Proportion Who Donated by Treatment Group

```{python}
# | echo: false
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate donation rates
donation_rates = (
    karlan_list_2007_pretty.groupby("treatment")["gave"].mean().reset_index()
)

# Create barplot
plt.figure(figsize=(6, 4))
sns.barplot(x="treatment", y="gave", data=donation_rates)
plt.ylabel("Proportion Donated")
plt.xlabel("Group (0 = Control, 1 = Treatment)")
plt.title("Donation Rate by Treatment Group")
plt.ylim(0, 0.03)
plt.grid(axis="y")
plt.show()
```

In this bar plot, each bar shows the proportion of people in the treatment or control group who made a donation, illustrating the immediate difference in participation rates.

Histogram – Donation Amounts (Among Donors Only)

```{python}
# | echo: false
# Filter to donors only
donors = karlan_list_2007_pretty[karlan_list_2007_pretty["gave"] == 1]

# Plot histograms
plt.figure(figsize=(10, 4))

for i, group in enumerate([0, 1]):
    plt.subplot(1, 2, i + 1)
    sns.histplot(donors[donors["treatment"] == group]["amount"], bins=30, kde=False)
    plt.axvline(
        donors[donors["treatment"] == group]["amount"].mean(),
        color="red",
        linestyle="--",
    )
    plt.title(f"{'Control' if group == 0 else 'Treatment'} Group")
    plt.xlabel("Donation Amount")
    plt.ylabel("Frequency")

plt.tight_layout()
plt.show()
```

Here, the histogram reveals the distribution of how much donors gave, helping us detect outliers, skewness, or other patterns in giving behavior.

### Balance Test 

As an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.

As part of the balance test, I conducted a hand-computed two-sample t-test to determine whether the treatment and control groups differed significantly on the variable mrm2 (months since last donation). The results showed that the treatment group had a mean of 13.012 months, while the control group had a mean of 12.998 months. The calculated t-statistic was 0.120 with an associated p-value of 0.9049. Since the p-value is well above the 0.05 threshold, we fail to reject the null hypothesis and conclude that there is no statistically significant difference between the groups. This result supports the validity of the randomization mechanism, as it suggests that both groups were balanced on this pre-treatment variable.

```{python}
# | echo: false
# We'll do a hand-computed two-sample t-test for "mrm2"
# using the formula from the class slides:
#
#         t = (X̄_treatment - X̄_control)
#             --------------------------
#             sqrt((s_t^2 / n_t) + (s_c^2 / n_c))
#
# We'll then derive a two-sided p-value from that t-statistic.

import pandas as pd
import math
from math import sqrt
from scipy.stats import t

# 1) Load file (already done above)

# 2) Separate "mrm2" for treatment (1) and control (0)
mrm2_treatment = karlan_list_2007_pretty.loc[
    karlan_list_2007_pretty["treatment"] == 1, "mrm2"
].dropna()
mrm2_control = karlan_list_2007_pretty.loc[
    karlan_list_2007_pretty["treatment"] == 0, "mrm2"
].dropna()

# 3) Calculate sample sizes
n_t = len(mrm2_treatment)
n_c = len(mrm2_control)

# 4) Calculate sample means
Xbar_t = mrm2_treatment.mean()
Xbar_c = mrm2_control.mean()

# 5) Calculate sample variances (unbiased sample variance)
s_t2 = mrm2_treatment.var()  # ddof=1 by default
s_c2 = mrm2_control.var()

# 6) Compute the t-value using the manual formula (Welch's approach for unequal var)
numerator = Xbar_t - Xbar_c
denominator = math.sqrt((s_t2 / n_t) + (s_c2 / n_c))
t_stat = numerator / denominator

# 7) Approximate degrees of freedom (Welch-Satterthwaite)
num_df = ((s_t2 / n_t) + (s_c2 / n_c)) ** 2
den_df = ((s_t2 / n_t) ** 2 / (n_t - 1)) + ((s_c2 / n_c) ** 2 / (n_c - 1))
df_approx = num_df / den_df

# 8) Two-sided p-value
p_value = 2.0 * (1.0 - t.cdf(abs(t_stat), df_approx))

# 9) Print results
print("Hand-Composed Two-Sample t-Test for mrm2")
print("=========================================")
print(f"Treatment Mean (mrm2): {Xbar_t:.3f}, n={n_t}")
print(f"Control Mean (mrm2):   {Xbar_c:.3f}, n={n_c}")
print(f"t-statistic:           {t_stat:.3f}")
print(f"Degrees of freedom:    {df_approx:.2f}")
print(f"p-value (two-sided):   {p_value:.4g}")
```

To validate these results using a different method, I also ran a simple linear regression where mrm2 was regressed on the treatment variable. This approach tests the same hypothesis as the t-test—that the average months since last donation is equal across groups. The estimated treatment effect (0.014) is nearly identical to the difference in group means, and the associated p-value (0.905) confirms the same conclusion: there is no statistically significant difference between the groups. This reinforces the finding that the randomization successfully created balanced groups.

```{python}
# | echo: false
# Run a regression of mrm2 on treatment using rsm.model.regress

import pyrsm as rsm

# Run the model (mrm2 ~ treatment)
reg_mrm2 = rsm.regress(
    data={"karlan_list_2007_pretty": karlan_list_2007_pretty},
    rvar="mrm2",  # Response variable (target)
    evar=["treatment"],  # Explanatory variable (binary group)
)

# Show the regression summary
reg_mrm2.summary()
```

As part of the balance test, I conducted a hand-computed two-sample t-test to determine whether the treatment and control groups differed significantly on the variable freq (number of prior donations). This represents the second variable tested for robustness, following the initial test on mrm2 (months since last donation). The results showed that the treatment group had a mean of 8.035 donations, while the control group had a mean of 8.047 donations. The calculated t-statistic was -0.111 with an associated p-value of 0.9117. Since the p-value is well above the 0.05 threshold, we fail to reject the null hypothesis and conclude that there is no statistically significant difference between the groups. The fact that both mrm2 and freq are balanced across treatment and control groups further validates the randomization mechanism, confirming that the two groups are comparable on key pre-treatment characteristics.

```{python}
# | echo: false
# We'll do a hand-computed two-sample t-test for "freq"
# using the formula from the class slides:
#
#         t = (X̄_treatment - X̄_control)
#             --------------------------
#             sqrt((s_t^2 / n_t) + (s_c^2 / n_c))
#
# We'll then derive a two-sided p-value from that t-statistic.

import math
from math import sqrt
from scipy.stats import t

# 1) Separate "freq" for treatment (1) and control (0)
freq_treatment = karlan_list_2007_pretty.loc[
    karlan_list_2007_pretty["treatment"] == 1, "freq"
].dropna()
freq_control = karlan_list_2007_pretty.loc[
    karlan_list_2007_pretty["treatment"] == 0, "freq"
].dropna()

# 2) Calculate sample sizes
n_t = len(freq_treatment)
n_c = len(freq_control)

# 3) Calculate sample means
Xbar_t = freq_treatment.mean()
Xbar_c = freq_control.mean()

# 4) Calculate sample variances (unbiased sample variance)
s_t2 = freq_treatment.var()
s_c2 = freq_control.var()

# 5) Compute the t-value using the manual formula (Welch's approach)
numerator = Xbar_t - Xbar_c
denominator = math.sqrt((s_t2 / n_t) + (s_c2 / n_c))
t_stat = numerator / denominator

# 6) Approximate degrees of freedom (Welch-Satterthwaite)
num_df = ((s_t2 / n_t) + (s_c2 / n_c)) ** 2
den_df = ((s_t2 / n_t) ** 2 / (n_t - 1)) + ((s_c2 / n_c) ** 2 / (n_c - 1))
df_approx = num_df / den_df

# 7) Two-sided p-value
p_value = 2.0 * (1.0 - t.cdf(abs(t_stat), df_approx))

# 8) Print results
print("Hand-Composed Two-Sample t-Test for freq")
print("=========================================")
print(f"Treatment Mean (freq): {Xbar_t:.3f}, n={n_t}")
print(f"Control Mean (freq):   {Xbar_c:.3f}, n={n_c}")
print(f"t-statistic:           {t_stat:.3f}")
print(f"Degrees of freedom:    {df_approx:.2f}")
print(f"p-value (two-sided):   {p_value:.4g}")
```

To validate the results from the t-test, I also performed a linear regression of freq on the treatment variable. Like the t-test, this regression assesses whether there is a statistically significant difference in the number of prior donations between treatment and control groups. The coefficient on treatment (-0.012) closely matches the difference in group means, and the p-value (0.912) confirms the same conclusion: no significant difference exists. This consistency between the regression and the hand-calculated t-test reinforces the finding that the treatment assignment did not systematically influence pre-treatment donation frequency.

```{python}
# | echo: false
# Run a regression of freq on treatment using rsm.model.regress

import pyrsm as rsm

# Run the model (freq ~ treatment)
reg_freq = rsm.regress(
    data={"karlan_list_2007_pretty": karlan_list_2007_pretty},
    rvar="freq",  # Response variable (target)
    evar=["treatment"],  # Explanatory variable (binary group)
)

# Show the regression summary
reg_freq.summary()
```

These results conclude the Balance Test section and provide strong support for the success of the randomization mechanism.

## Experimental Results

### Charitable Contribution Made

First, I analyze whether matched donations lead to an increased response rate of making a donation. 

Here’s a short paragraph you can use to replace the todo sentence in the screenshot:

The bar plot below shows the proportion of individuals who made a donation in the treatment and control groups. This visualization offers an early look at the potential impact of being assigned to the treatment group, with a slightly higher donation rate observed. While this plot conveys similar information to the one presented in the Data section, it is revisited here in the context of hypothesis testing to begin assessing whether the observed difference is statistically significant.

```{python}
# | echo: false
# Barplot showing proportion of people who donated (treatment vs control)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Calculate donation rates by group
donation_rates = (
    karlan_list_2007_pretty.groupby("treatment")["gave"].mean().reset_index()
)

# Plot the barplot
plt.figure(figsize=(6, 4))
sns.barplot(x="treatment", y="gave", data=donation_rates)
plt.title("Proportion Who Donated by Treatment Group")
plt.xlabel("Group (0 = Control, 1 = Treatment)")
plt.ylabel("Proportion Donated")
plt.ylim(0, 0.03)
plt.grid(axis="y")
plt.tight_layout()
plt.show()
```

Based on the code used in our earlier balance checks, I ran a t‐test comparing the proportion who donated (gave == 1) across the treatment and control groups. The results show that the treatment group’s average donation rate is modestly but meaningfully higher than the control group’s rate, closely mirroring the figures in Karlan and List’s Table 2a Panel A. The p‐value from this test is well below the usual 5 percent threshold, indicating that the difference is unlikely to be by chance. In real terms, such a small bump in donation rates can significantly boost total contributions, demonstrating that even a simple intervention like a matching grant can alter donor behavior. This supports the broader conclusion that small “price” or matching signals can nudge people to act, thereby increasing charitable giving.

Here’s a clear paragraph you can use to summarize the probit regression results in context with Table 3, Column 1 from the Karlan and List (2007) paper:

I ran a probit regression to estimate whether being assigned to the treatment group significantly increased the likelihood of making a charitable donation. The model used gave as the binary outcome and treatment as the explanatory variable. The results show a positive and statistically significant coefficient of 0.0868 (p = 0.002), indicating that assignment to the treatment group is associated with a higher probability of donating. However, this estimate does not match the coefficient reported in Table 3, Column 1 of the original paper, which shows a much smaller effect size of 0.004 with a standard error of 0.001. Despite the model setup appearing consistent, the difference in results suggests there may be differences in the underlying implementation or sample filtering. Still, the significance of the treatment variable aligns with the paper’s broader conclusion: matching grants, even modest ones, can meaningfully shift donation behavior.

```{python}
# | echo: false
# Run a probit regression: gave ~ treatment
# This tests whether being assigned to the treatment group changes the likelihood of making a donation

import statsmodels.api as sm
import statsmodels.formula.api as smf

# Step 1: Fit the probit model
# 'gave' is a binary variable (1 = donated, 0 = didn't donate)
# 'treatment' is the binary explanatory variable
probit_model = smf.probit("gave ~ treatment", data=karlan_list_2007_pretty).fit()

# Step 2: Show a summary of the results
probit_model.summary()
```

### Differences between Match Rates

Next, I assess the effectiveness of different sizes of matched donations on the response rate.

_todo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the "figures suggest" comment the authors make on page 8?_

```{python}
# | echo: false
# We'll run t-tests to compare the proportion donating across different match ratio groups
#
# For example, does 2:1 lead to a higher 'gave' rate than 1:1?

import pandas as pd
from scipy.stats import ttest_ind

# 1) Define subsets by ratio group
#    (Adjust your queries if your 'ratio' column stores numeric vs. string values)
df_ratio_1_1 = karlan_list_2007_pretty.query("ratio == '1:1'")
df_ratio_2_1 = karlan_list_2007_pretty.query("ratio == '2:1'")
df_ratio_3_1 = karlan_list_2007_pretty.query("ratio == '3:1'")

# 2) Define a helper function for Welch's two-sample t-test on 'gave'
def compare_ratios(df_a, df_b, label_a, label_b):
    """
    Runs a Welch t-test comparing the proportion donating in two ratio groups.
    df_a, df_b: data subsets
    label_a, label_b: strings for printing
    """
    # Extract the binary outcome arrays
    gave_a = df_a["gave"].dropna()
    gave_b = df_b["gave"].dropna()
    
    # Perform Welch's t-test (equal_var=False)
    t_stat, p_val = ttest_ind(gave_a, gave_b, equal_var=False)
    
    print(f"=== Comparing ratio {label_a} vs ratio {label_b} ===")
    print(f"Mean(gave) for {label_a}: {gave_a.mean():.4f}  (n={len(gave_a)})")
    print(f"Mean(gave) for {label_b}: {gave_b.mean():.4f}  (n={len(gave_b)})")
    print(f"T-statistic: {t_stat:.4f} | P-value: {p_val:.4g}")
    print("--------------------------------------\n")

# 3) Compare 2:1 vs 1:1
compare_ratios(df_ratio_2_1, df_ratio_1_1, "2:1", "1:1")

# Compare 3:1 vs 1:1
compare_ratios(df_ratio_3_1, df_ratio_1_1, "3:1", "1:1")

# Optionally, compare 2:1 vs 3:1
compare_ratios(df_ratio_2_1, df_ratio_3_1, "2:1", "3:1")
```

_todo: Assess the same issue using a regression. Specifically, create the variable `ratio1` then regress `gave` on `ratio1`, `ratio2`, and `ratio3` (or alternatively, regress `gave` on the categorical variable `ratio`). Interpret the coefficients and their statistical precision._

_todo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios.  Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?_


### Size of Charitable Contribution

In this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.

_todo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?_

_todo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients -- what did we learn? Does the treatment coefficient have a causal interpretation?_ 

_todo: Make two plots: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot._


## Simulation Experiment

As a reminder of how the t-statistic "works," in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.

Suppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. 

Further suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.

### Law of Large Numbers

_to do:  Simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. You'll then calculate a vector of 10,000 differences, and then you'll plot the cumulative average of that vector of differences. This average will likely be "noisey" when only averaging a few numbers, but should "settle down" and approximate the treatment effect (0.004 = 0.022 - 0.018) as the sample size gets large. Explain the chart to the reader._


### Central Limit Theorem

_to do: Make 4 histograms at sample sizes 50, 200, 500, and 1000.  To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. The repeat for the other 3 histograms. Explain this sequence of histograms and its relationship to the central limit theorem to the reader._

