<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.5">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Juan Hernández Guizar">
<meta name="dcterms.date" content="2025-05-28">
<meta name="description" content="Assignment 3">

<title>Multi-Nomial Logit (MNL) &amp; Conjoint – Juan Hernández Guizar</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ff4371ef257df69894857e99c6ad0d06.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-f735cf07a6910d811a1cbb5bbbcb55bb.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Juan Hernández Guizar</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text"><i class="bi bi-house"></i> Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Resume.html"> 
<span class="menu-text"><i class="bi bi-newspaper"></i> Resume</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text"><i class="bi bi-pencil"></i> Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.youtube.com/@CreaCircuitos"> <i class="bi bi-youtube" role="img">
</i> 
<span class="menu-text">YouTube Channel</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://rsm-aburda.github.io/MGTA-452-Final-Project/index.html"> <i class="bi bi-spotify" role="img">
</i> 
<span class="menu-text">Spotify Group Project</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#likelihood-for-the-multi-nomial-logit-mnl-model" id="toc-likelihood-for-the-multi-nomial-logit-mnl-model" class="nav-link active" data-scroll-target="#likelihood-for-the-multi-nomial-logit-mnl-model">1. Likelihood for the Multi-nomial Logit (MNL) Model</a></li>
  <li><a href="#simulate-conjoint-data" id="toc-simulate-conjoint-data" class="nav-link" data-scroll-target="#simulate-conjoint-data">2. Simulate Conjoint Data</a></li>
  <li><a href="#preparing-the-data-for-estimation" id="toc-preparing-the-data-for-estimation" class="nav-link" data-scroll-target="#preparing-the-data-for-estimation">3. Preparing the Data for Estimation</a></li>
  <li><a href="#estimation-via-maximum-likelihood" id="toc-estimation-via-maximum-likelihood" class="nav-link" data-scroll-target="#estimation-via-maximum-likelihood">4. Estimation via Maximum Likelihood</a></li>
  <li><a href="#estimation-via-bayesian-methods" id="toc-estimation-via-bayesian-methods" class="nav-link" data-scroll-target="#estimation-via-bayesian-methods">5. Estimation via Bayesian Methods</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">6. Discussion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Multi-Nomial Logit (MNL) &amp; Conjoint</h1>
</div>

<div>
  <div class="description">
    Assignment 3
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Juan Hernández Guizar </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 28, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm.</p>
<section id="likelihood-for-the-multi-nomial-logit-mnl-model" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-for-the-multi-nomial-logit-mnl-model">1. Likelihood for the Multi-nomial Logit (MNL) Model</h2>
<p>Suppose we have <span class="math inline">\(i=1,\ldots,n\)</span> consumers who each select exactly one product <span class="math inline">\(j\)</span> from a set of <span class="math inline">\(J\)</span> products. The outcome variable is the identity of the product chosen <span class="math inline">\(y_i \in \{1, \ldots, J\}\)</span> or equivalently a vector of <span class="math inline">\(J-1\)</span> zeros and <span class="math inline">\(1\)</span> one, where the <span class="math inline">\(1\)</span> indicates the selected product. For example, if the third product was chosen out of 3 products, then either <span class="math inline">\(y=3\)</span> or <span class="math inline">\(y=(0,0,1)\)</span> depending on how we want to represent it. Suppose also that we have a vector of data on each product <span class="math inline">\(x_j\)</span> (eg, brand, price, etc.).</p>
<p>We model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:</p>
<p><span class="math display">\[ U_{ij} = x_j'\beta + \epsilon_{ij} \]</span></p>
<p>where <span class="math inline">\(\epsilon_{ij}\)</span> is an i.i.d. extreme value error term.</p>
<p>The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer <span class="math inline">\(i\)</span> chooses product <span class="math inline">\(j\)</span>:</p>
<p><span class="math display">\[ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} \]</span></p>
<p>For example, if there are 3 products, the probability that consumer <span class="math inline">\(i\)</span> chooses product 3 is:</p>
<p><span class="math display">\[ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta}} \]</span></p>
<p>A clever way to write the individual likelihood function for consumer <span class="math inline">\(i\)</span> is the product of the <span class="math inline">\(J\)</span> probabilities, each raised to the power of an indicator variable (<span class="math inline">\(\delta_{ij}\)</span>) that indicates the chosen product:</p>
<p><span class="math display">\[ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}\]</span></p>
<p>Notice that if the consumer selected product <span class="math inline">\(j=3\)</span>, then <span class="math inline">\(\delta_{i3}=1\)</span> while <span class="math inline">\(\delta_{i1}=\delta_{i2}=0\)</span> and the likelihood is:</p>
<p><span class="math display">\[ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^3e^{x_k'\beta}} \]</span></p>
<p>The joint likelihood (across all consumers) is the product of the <span class="math inline">\(n\)</span> individual likelihoods:</p>
<p><span class="math display">\[ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} \]</span></p>
<p>And the joint log-likelihood function is:</p>
<p><span class="math display">\[ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) \]</span></p>
</section>
<section id="simulate-conjoint-data" class="level2">
<h2 class="anchored" data-anchor-id="simulate-conjoint-data">2. Simulate Conjoint Data</h2>
<p>We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.</p>
<p>Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.</p>
<p>The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer <span class="math inline">\(i\)</span> for hypothethical streaming service <span class="math inline">\(j\)</span> is</p>
<p><span class="math display">\[
u_{ij} = (1 \times Netflix_j) + (0.5 \times Prime_j) + (-0.8*Ads_j) - 0.1\times Price_j + \varepsilon_{ij}
\]</span></p>
<p>where the variables are binary indicators and <span class="math inline">\(\varepsilon\)</span> is Type 1 Extreme Value (ie, Gumble) distributed.</p>
<p>To visualize this we developed a simulation of the conjoin data below.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="cell-setup" class="cell" data-message="false" data-execution_count="1">
<div id="setup" class="cell-output cell-output-display" data-execution_count="1">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">resp</th>
<th data-quarto-table-cell-role="th">task</th>
<th data-quarto-table-cell-role="th">brand</th>
<th data-quarto-table-cell-role="th">ad</th>
<th data-quarto-table-cell-role="th">price</th>
<th data-quarto-table-cell-role="th">choice</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1</td>
<td>1</td>
<td>P</td>
<td>No</td>
<td>32</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1</td>
<td>1</td>
<td>N</td>
<td>No</td>
<td>28</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1</td>
<td>1</td>
<td>N</td>
<td>No</td>
<td>24</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1</td>
<td>2</td>
<td>H</td>
<td>No</td>
<td>28</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>1</td>
<td>2</td>
<td>H</td>
<td>No</td>
<td>8</td>
<td>1</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="inspect" class="cell" data-execution_count="2">
<div class="cell-output cell-output-stdout">
<pre><code>The simulated dataset contains 3000 rows and 6 columns (exactly 100 respondents × 10 tasks × 3 alternatives). Just as expected!</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="preparing-the-data-for-estimation" class="level2">
<h2 class="anchored" data-anchor-id="preparing-the-data-for-estimation">3. Preparing the Data for Estimation</h2>
<p>The “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer <span class="math inline">\(i\)</span>, covariate <span class="math inline">\(k\)</span>, and product <span class="math inline">\(j\)</span>) instead of the typical 2 dimensions for cross-sectional regression models (consumer <span class="math inline">\(i\)</span> and covariate <span class="math inline">\(k\)</span>). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.</p>
<p>Below, we load the conjoint dataset and reshape it for estimation. We create dummy variables for the brand and ad features (using Hulu and “No Ads” as the base levels), and we construct a task identifier to group alternatives belonging to the same choice set. We then display the first few rows to verify the structure:</p>
<div id="0c6ac104" class="cell" data-execution_count="3">
<div class="cell-output cell-output-stdout">
<pre><code>   resp  task  choice brand   ad  price  brand_N  brand_P  ad_yes  task_id
0     1     1       1     N  Yes     28        1        0       1        0
1     1     1       0     H  Yes     16        0        0       1        0
2     1     1       0     P  Yes     16        0        1       1        0
3     1     2       0     N  Yes     32        1        0       1        1
4     1     2       1     P  Yes     16        0        1       1        1</code></pre>
</div>
</div>
<p>We can see that each choice task (task_id) contains three rows (one per alternative). For example, resp=1, task=1 (first three rows) had alternatives from brands N, H, P all with ads (“Yes”) and different prices, and the first row (brand=N, price=28) was marked choice=1 as the selected option (it had the highest simulated utility). The dataset is now ready for model estimation.</p>
</section>
<section id="estimation-via-maximum-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="estimation-via-maximum-likelihood">4. Estimation via Maximum Likelihood</h2>
<p>With the data prepared, we now turn to estimating the MNL model parameters by Maximum Likelihood. We have four parameters to estimate: <span class="math inline">\(\beta_{\text{Netflix}}\)</span> and <span class="math inline">\(\beta_{\text{Prime}}\)</span> for the two non-baseline brands (Hulu is the baseline, so its effect is 0 by construction), <span class="math inline">\(\beta_{\text{Ads}}\)</span> for the effect of having ads (versus no ads), and <span class="math inline">\(\beta_{\text{Price}}\)</span> for the price coefficient.</p>
<p>Log-Likelihood Function: First, we need to code up the log-likelihood function for the MNL. Using the data, the log-likelihood <span class="math inline">\(\ell_n(\beta)\)</span> can be computed by summing, over all choice tasks, the log of the probability of the chosen alternative. Given our data structure, a convenient method is: for each choice task, find the linear utility <span class="math inline">\(v_{ij} = x_j’ \beta\)</span> for each alternative, compute the choice probabilities <span class="math inline">\(\mathbb{P}_i(j)\)</span> via the softmax formula, then accumulate <span class="math inline">\(\log \mathbb{P}_i(j^)\)</span> for the chosen alternative <span class="math inline">\(j^{*}\)</span>. We implement this below. To speed up computation, we vectorize the operations using NumPy:</p>
<div id="1defcda9" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract numpy arrays for faster computation</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[[<span class="st">"brand_N"</span>, <span class="st">"brand_P"</span>, <span class="st">"ad_yes"</span>, <span class="st">"price"</span>]].values  <span class="co"># Feature matrix (3000 x 4)</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>group_ids <span class="op">=</span> df[<span class="st">"task_id"</span>].values  <span class="co"># Task identifiers (length 3000)</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>choice <span class="op">=</span> df[<span class="st">"choice"</span>].values  <span class="co"># Chosen indicator (0/1 for each row)</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the log-likelihood function for parameters beta (as a NumPy array)</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loglik(beta):</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> np.array(beta)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute linear utility v = X * beta for all alternatives</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> X.dot(beta)  <span class="co"># shape (3000,)</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute exp(v) and sum exp(v) by task (denominator of softmax)</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    exp_v <span class="op">=</span> np.exp(v)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sum of exp(v) for each task (using group_ids to aggregate)</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    sum_exp_v_by_task <span class="op">=</span> np.bincount(</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        group_ids, weights<span class="op">=</span>exp_v, minlength<span class="op">=</span>df[<span class="st">"task_id"</span>].nunique()</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sum of v for the chosen alternative in each task (only one chosen per task)</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    chosen_v_by_task <span class="op">=</span> np.bincount(</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        group_ids, weights<span class="op">=</span>v <span class="op">*</span> choice, minlength<span class="op">=</span>df[<span class="st">"task_id"</span>].nunique()</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Log-likelihood is sum over tasks of (v_chosen - log(sum_exp_v))</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    log_lik_value <span class="op">=</span> np.<span class="bu">sum</span>(chosen_v_by_task <span class="op">-</span> np.log(sum_exp_v_by_task))</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log_lik_value</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Quick sanity check: compute log-likelihood at the true beta values used in simulation</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>beta_true <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.8</span>, <span class="op">-</span><span class="fl">0.1</span>])</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Log-likelihood at true beta: </span><span class="sc">{</span>loglik(beta_true)<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Log-likelihood at true beta: -880.344</code></pre>
</div>
</div>
<p>We included a quick check: plugging in the true part-worths [1.0, 0.5, -0.8, -0.1] gives a log-likelihood of roughly -880.3. Now, we will let the computer search for the <span class="math inline">\(\beta\)</span> that maximizes the log-likelihood. In practice, we maximize the likelihood by minimizing the negative log-likelihood. We can use a numerical optimizer (from SciPy in Python) to find the Maximum Likelihood Estimates (MLEs) of <span class="math inline">\(\beta\)</span>. We also compute the Hessian-based standard errors and 95% confidence intervals for these estimates. The table below summarizes the results:</p>
<div id="9ba21602" class="cell" data-execution_count="5">
<div class="cell-output cell-output-stdout">
<pre><code>Parameter  Estimate  Std. Error 95% Confidence Interval
β_Netflix     0.941       0.114          [0.717, 1.165]
  β_Prime     0.502       0.121          [0.265, 0.738]
    β_Ads    -0.732       0.089        [-0.906, -0.558]
  β_Price    -0.099       0.006        [-0.112, -0.087]</code></pre>
</div>
</div>
<p>All four estimates are very close to the true values used in the simulation, which is reassuring. The estimate for β_Netflix is about 0.94 (true was 1.0) and for β_Prime about 0.50 (true 0.5), with Hulu as the baseline (so Hulu’s implicit β is 0). This means that, holding ads and price constant, a Netflix offering has about 0.94 higher utility units than an otherwise identical Hulu offering, and Prime has 0.50 higher utility than Hulu. The Ads coefficient is -0.732, indicating a strong negative effect of having advertisements: an offering with ads is less attractive by ~0.73 utility units compared to an ad-free equivalent. The Price coefficient is -0.099, meaning each additional $1 per month reduces utility by ~0.099. All parameters are significantly different from zero at the 95% confidence level (zero lies outside all the confidence intervals), aligning with our expectations (e.g., higher price and ads included both significantly reduce the likelihood of choice).</p>
</section>
<section id="estimation-via-bayesian-methods" class="level2">
<h2 class="anchored" data-anchor-id="estimation-via-bayesian-methods">5. Estimation via Bayesian Methods</h2>
<p>Next, we estimate the MNL model via a Bayesian approach. Rather than finding a single best estimate as in MLE, Bayesian inference will produce a posterior distribution for the parameters. We use a Metropolis-Hastings (M-H) algorithm (a type of Markov Chain Monte Carlo) to sample from the posterior distribution of <span class="math inline">\(\beta\)</span>.</p>
<p>Priors: We choose relatively non-informative (weak) priors for the parameters. For the binary feature coefficients (<span class="math inline">\(\beta_{\text{Netflix}}, \beta_{\text{Prime}}, \beta_{\text{Ads}}\)</span>) we use independent priors <span class="math inline">\(\mathcal{N}(0,;5)\)</span> – a normal distribution with mean 0 and variance 5 (standard deviation <span class="math inline">\(\approx 2.236\)</span>). For the price coefficient <span class="math inline">\(\beta_{\text{Price}}\)</span>, we use a slightly more informative prior <span class="math inline">\(\mathcal{N}(0,;1)\)</span> (mean 0, variance 1) since price effects are often tighter in range. These priors still cover a wide range of plausible values relative to our expected magnitudes, essentially adding only a gentle regularization around 0.</p>
<p>MCMC Setup: We will run the M-H sampler for 11,000 iterations and discard the first 1,000 draws as “burn-in” to allow the chain to converge. This will leave us with 10,000 posterior draws for inference. We construct a symmetric proposal distribution as a multivariate normal with no covariance between parameters (diagonal covariance matrix). In particular, we use independent normal proposal steps with standard deviations: 0.05 for each of <span class="math inline">\(\beta_{\text{Netflix}}, \beta_{\text{Prime}}, \beta_{\text{Ads}}\)</span>, and 0.005 for <span class="math inline">\(\beta_{\text{Price}}\)</span>. These step sizes (chosen based on the scale of the data and priors) should yield a reasonable acceptance rate for the M-H algorithm.</p>
<p>Below is the Python implementation of the Metropolis-Hastings sampler for the posterior of <span class="math inline">\(\beta\)</span>:</p>
<div id="bf9acba3" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define log-posterior function (log-likelihood + log-prior)</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_posterior(beta):</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># log-likelihood from data:</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    log_lik_val <span class="op">=</span> loglik(beta)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># log-prior for each parameter:</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prior variances: 5 for Netflix/Prime/Ads, 1 for Price</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> np.array(beta)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Log-prior for three binary attribute betas ~ N(0,5)</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    var_bin <span class="op">=</span> <span class="fl">5.0</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    log_prior_bin <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> (</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        <span class="dv">3</span> <span class="op">*</span> math.log(<span class="dv">2</span> <span class="op">*</span> math.pi <span class="op">*</span> var_bin) <span class="op">+</span> np.<span class="bu">sum</span>(beta[:<span class="dv">3</span>] <span class="op">**</span> <span class="dv">2</span>) <span class="op">/</span> var_bin</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Log-prior for price beta ~ N(0,1)</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    var_price <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    log_prior_price <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> (</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        math.log(<span class="dv">2</span> <span class="op">*</span> math.pi <span class="op">*</span> var_price) <span class="op">+</span> (beta[<span class="dv">3</span>] <span class="op">**</span> <span class="dv">2</span>) <span class="op">/</span> var_price</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log_lik_val <span class="op">+</span> log_prior_bin <span class="op">+</span> log_prior_price</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="co"># M-H sampling parameters</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> <span class="dv">11000</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>burn_in <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Proposal distribution standard deviations for [Netflix, Prime, Ads, Price]</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>proposal_sd <span class="op">=</span> np.array([<span class="fl">0.05</span>, <span class="fl">0.05</span>, <span class="fl">0.05</span>, <span class="fl">0.005</span>])</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize chain and starting value (use MLE as starting point for efficiency)</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>chain <span class="op">=</span> np.zeros((iterations, <span class="dv">4</span>))</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>chain[<span class="dv">0</span>] <span class="op">=</span> beta_hat  <span class="co"># start at MLE estimate (could also start at [0,0,0,0])</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>current_beta <span class="op">=</span> chain[<span class="dv">0</span>].copy()</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>current_logpost <span class="op">=</span> log_posterior(current_beta)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>accept_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, iterations):</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Propose a new beta by adding random normal noise to each dimension</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    proposal <span class="op">=</span> current_beta <span class="op">+</span> np.random.normal(<span class="dv">0</span>, proposal_sd, size<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>    prop_logpost <span class="op">=</span> log_posterior(proposal)</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Metropolis-Hastings acceptance probability</span></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>    log_accept_ratio <span class="op">=</span> prop_logpost <span class="op">-</span> current_logpost</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> math.log(np.random.rand()) <span class="op">&lt;</span> log_accept_ratio:</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Accept proposal</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>        current_beta <span class="op">=</span> proposal</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>        current_logpost <span class="op">=</span> prop_logpost</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>        chain[t] <span class="op">=</span> proposal</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>        accept_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reject proposal (retain current beta)</span></span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>        chain[t] <span class="op">=</span> current_beta</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>accept_rate <span class="op">=</span> accept_count <span class="op">/</span> (iterations <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Acceptance rate: </span><span class="sc">{</span>accept_rate<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Acceptance rate: 0.566</code></pre>
</div>
</div>
<p>We print the acceptance rate to ensure the sampler is moving adequately. In this run, the acceptance rate is around 50-60%, indicating the proposal step sizes are reasonable for efficient mixing (neither too high nor too low).</p>
<p>After discarding the first 1,000 draws as burn-in, we use the remaining 10,000 posterior draws to analyze the posterior distribution of each parameter. To illustrate convergence and the posterior distribution, the following figures show the trace and the histogram of the posterior sample for one of the parameters (here we choose <span class="math inline">\(\beta_{\text{Netflix}}\)</span> as an example):</p>
<div id="12fb15f6" class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Assignment_3_files/figure-html/cell-8-output-1.png" width="518" height="377" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Trace plot of the MCMC chain for <span class="math inline">\(\beta_{\text{Netflix}}\)</span> after burn-in. The chain fluctuates around its mean (red dashed line), indicating good mixing.</p>
<div id="16593fe0" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Assignment_3_files/figure-html/cell-9-output-1.png" width="527" height="379" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Histogram of the posterior draws for <span class="math inline">\(\beta_{\text{Netflix}}\)</span>. The distribution is approximately normal. The red line marks the posterior mean, and the black dotted lines mark the 95% credible interval bounds.</p>
<p>Using the posterior sample, we can summarize the Bayesian estimates for all four parameters. Below we report the posterior mean, standard deviation, and 95% credible interval for each <span class="math inline">\(\beta\)</span>, and compare them to the earlier MLE results: • <strong><span class="math inline">\(\beta_{\text{Netflix}}\)</span>: Posterior mean = 0.941, SD = 0.112, 95% credible interval [0.721, 1.157]. • </strong><span class="math inline">\(\beta_{\text{Prime}}\)</span>: Posterior mean = 0.502, SD = 0.113, 95% credible interval [0.277, 0.723]. • <strong><span class="math inline">\(\beta_{\text{Ads}}\)</span>: Posterior mean = -0.734, SD = 0.088, 95% credible interval [-0.907, -0.566]. • </strong><span class="math inline">\(\beta_{\text{Price}}\)</span>: Posterior mean = -0.100, SD = 0.006, 95% credible interval [-0.112, -0.088].</p>
<p>These posterior estimates are virtually identical to the MLE results we obtained earlier. The weak priors did not substantially pull the estimates toward zero, so the posterior means (0.941, 0.502, -0.734, -0.100) are almost the same as the MLE point estimates (0.941, 0.502, -0.732, -0.099). Likewise, the posterior standard deviations are in line with the MLE standard errors, and the 95% credible intervals correspond closely to the 95% confidence intervals from the likelihood approach. This consistency provides a nice validation: given our large sample (1000 choice tasks) and relatively uninformative priors, the Bayesian and frequentist approaches yield the same substantive conclusions.</p>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">6. Discussion</h2>
<p><em>todo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does <span class="math inline">\(\beta_\text{Netflix} &gt; \beta_\text{Prime}\)</span> mean? Does it make sense that <span class="math inline">\(\beta_\text{price}\)</span> is negative?</em></p>
<p><em>todo: At a high level, discuss what change you would need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze “real world” conjoint data.</em></p>
<p>If we consider these results as if they came from a real conjoint study (i.e.&nbsp;not knowing the “true” simulation values), we can interpret the parameter estimates in practical terms. We observe that <span class="math inline">\(\beta_{\text{Netflix}}\)</span> (approximately 0.94) is larger than <span class="math inline">\(\beta_{\text{Prime}}\)</span> (about 0.50). This indicates that, on average, consumers in the study derive more utility from the Netflix brand than from Amazon Prime, with Hulu as the baseline (zero effect). In other words, Netflix is the most preferred streaming brand among the three, and Amazon Prime is also preferred to Hulu but less so than Netflix. The negative <span class="math inline">\(\beta_{\text{Ads}} \approx -0.73\)</span> confirms that including advertisements substantially lowers consumer utility compared to an ad-free experience. Similarly, <span class="math inline">\(\beta_{\text{Price}}\)</span> is about -0.10, meaning higher price has a negative effect on choice probability (which makes intuitive sense – consumers prefer cheaper options, all else equal).</p>
<!-- It is important to note that $\beta_{\text{Price}}$ being negative is not only sensible, but we can also quantify its meaning: a one-unit increase in utility is equivalent to about $10 in price (since $1 / 0.10 = 10$). Therefore, the brand coefficient for Netflix (0.94) can be interpreted as Netflix providing roughly $9–$10 worth of added value to consumers relative to Hulu, and Prime’s brand value is about $5 relative to Hulu. Likewise, having ads ($\beta_{\text{Ads}} \approx -0.73$) imposes a disutility roughly equivalent to $7–$8 per month in price. These insights are very useful for business decisions – for instance, they indicate how much more a company could potentially charge for a Netflix-branded service (or how much discount would be required to compensate for including ads). -->
<p>It is important to note that <span class="math inline">\(\beta_{\text{Price}}\)</span> being negative is not only sensible, but we can also quantify its meaning: a one-unit increase in utility is equivalent to about $10 in price (since <span class="math inline">\(1/0.10 = 10\)</span>). Therefore, the brand coefficient for Netflix (0.94) can be interpreted as Netflix providing roughly $9–$10 worth of added value to consumers relative to Hulu, and Prime’s brand value is about $5 relative to Hulu. Likewise, having ads (<span class="math inline">\(\beta_{\text{Ads}} \approx -0.73\)</span>) imposes a disutility roughly equivalent to $7–$8 per month in price. These insights are very useful for business decisions – for instance, they indicate how much more a company could potentially charge for a Netflix-branded service (or how much discount would be required to compensate for including ads).</p>
<p>Finally, how would we extend this to a multi-level (random-parameter) model? In a multi-level or hierarchical Bayesian conjoint model, we acknowledge that different consumers may have different preference parameters. Instead of one <span class="math inline">\(\beta\)</span> vector for the whole population, we assume each individual <span class="math inline">\(i\)</span> has their own <span class="math inline">\(\beta_i\)</span>, drawn from a population distribution (for example, <span class="math inline">\(\beta_i \sim \mathcal{N}(\mu, \Sigma)\)</span>). To simulate data from such a model, we would first draw each respondent’s true part-worths from a specified distribution (with some mean and variance to represent preference heterogeneity), and then use those individual-level <span class="math inline">\(\beta_i\)</span> to simulate each person’s choices. To estimate a hierarchical model, we would need to introduce additional layers in our estimation procedure – essentially estimating both the individual-level <span class="math inline">\(\beta_i\)</span> for each respondent and the hyperparameters (like the mean vector <span class="math inline">\(\mu\)</span> and covariance <span class="math inline">\(\Sigma\)</span> of the population distribution). This could be done via hierarchical Bayesian methods (where we would add Gibbs sampling or more complex MCMC steps to sample each <span class="math inline">\(\beta_i\)</span> and the hyperparameters) or via simulated maximum likelihood (also known as a mixed logit approach). In essence, the key change is moving from a fixed-effects MNL (one common <span class="math inline">\(\beta\)</span> for all) to a random-effects MNL where each consumer has their own <span class="math inline">\(\beta\)</span> drawn from a higher-level distribution. This addition would capture the real-world preference heterogeneity we expect in conjoint analysis, at the cost of a more complex simulation and estimation process.</p>
<p>We successfully implemented and compared maximum likelihood and Bayesian estimation for a multinomial logit conjoint model. Both approaches recovered the true part-worth parameters closely, reinforcing our understanding of MNL. From a business perspective, the results highlight clear patterns: brand matters (Netflix holds a strong advantage over competitors), ads are detrimental to user utility, and price sensitivity is significant. These findings imply that a streaming service can command a premium for a stronger brand or no-ad experience, whereas introducing ads or raising prices must be balanced against the substantial utility loss. By quantifying these trade-offs, conjoint analysis provides valuable guidance for product design and pricing strategy in the streaming industry.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>