[
  {
    "objectID": "blog/Assignment 4/Assignment_4.html",
    "href": "blog/Assignment 4/Assignment_4.html",
    "title": "Machine Learning",
    "section": "",
    "text": "todo: do two analyses. Do one of either 1a or 1b, AND one of either 2a or 2b."
  },
  {
    "objectID": "blog/Assignment 4/Assignment_4.html#a.-k-means",
    "href": "blog/Assignment 4/Assignment_4.html#a.-k-means",
    "title": "Machine Learning",
    "section": "1a. K-Means",
    "text": "1a. K-Means\ntodo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can “see” the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,…,7). What is the “right” number of clusters as suggested by these two metrics?\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this."
  },
  {
    "objectID": "blog/Assignment 4/Assignment_4.html#b.-latent-class-mnl",
    "href": "blog/Assignment 4/Assignment_4.html#b.-latent-class-mnl",
    "title": "Machine Learning",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "blog/Assignment 4/Assignment_4.html#a.-k-nearest-neighbors",
    "href": "blog/Assignment 4/Assignment_4.html#a.-k-nearest-neighbors",
    "title": "Machine Learning",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function – eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn’s KNeighborsClassifier in Python.\ntodo: run your function for k=1,…,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?"
  },
  {
    "objectID": "blog/Assignment 4/Assignment_4.html#b.-key-drivers-analysis",
    "href": "blog/Assignment 4/Assignment_4.html#b.-key-drivers-analysis",
    "title": "Machine Learning",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  },
  {
    "objectID": "blog/Assignment 4/Assignment_4.html#k-means-clustering",
    "href": "blog/Assignment 4/Assignment_4.html#k-means-clustering",
    "title": "Machine Learning",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\nK-Means is an unsupervised learning algorithm that groups unlabeled data into k clusters based on similarity. The goal is to partition the data so that points in the same cluster are more similar to each other than to those in other clusters ￼. In essence, K-Means tries to find cluster centers (called centroids) that minimize the distance of each point to its nearest centroid ￼.\nHow does K-Means work? At a high level, the algorithm follows an iterative refinement procedure ￼: • Initialize – Choose k initial centroids (often random picks from the data). • Assign – For each point, find the nearest centroid (by Euclidean distance) and assign the point to that cluster. • Update – Recompute each centroid as the average (mean) of all points assigned to it. • Repeat – Iterate the assign-update steps until centroids stop changing (convergence).\nThis process will partition the dataset into k clusters such that each point belongs to the cluster with the closest centroid. The algorithm stops when successive iterations no longer change the centroids (or change them negligibly), meaning the clustering has stabilized. The result is a set of clusters and their centroid locations.\nTo demonstrate K-Means, we’ll use the Palmer Penguins dataset, a popular alternative to the iris dataset. It contains measurements for three penguin species (Adelie, Chinstrap, Gentoo) from islands in Antarctica. We will use just two features for clustering: bill length and flipper length (both in mm). This gives us a 2D dataset that we can easily visualize. We will ignore the species labels during clustering (since K-Means is unsupervised), but it’s worth noting there are 3 true species in the data (which might correspond to 3 clusters).\nFirst, let’s load the dataset and take a peek at the data structure:\n\n# | echo: false\nimport pandas as pd\n\n# Load the Palmer Penguins dataset (CSV file provided)\ndf = pd.read_csv(\"Assignment_4_data/palmer_penguins.csv\")\n# print(df[[\"species\", \"bill_length_mm\", \"flipper_length_mm\"]].head())\n\n# Drop any rows with missing values in the features of interest\npenguins = df[[\"bill_length_mm\", \"flipper_length_mm\"]].dropna()\n# print(\"Dataset shape:\", penguins.shape)\n\nWe have 342 penguin observations with bill length and flipper length. Now, let’s implement the K-Means algorithm from scratch for a chosen number of clusters, K=3. (Choosing 3 is a reasonable guess here given the three species, but we will later analyze different k values.)\nWe’ll write a simple implementation of K-Means. The plan: 1. Randomly initialize 3 centroids by selecting 3 random points from the dataset. 2. Loop until convergence: • Compute the distance from each data point to each centroid. • Assign each point to the nearest centroid (forming 3 clusters). • Recompute each centroid as the mean of the points in its cluster. • If centroids don’t change (or change very little), break out.\nWe’ll also keep track of the cluster assignments at each iteration so we can visualize the progression.\n\n# | echo: false\nimport numpy as np\n\n# Prepare data as a numpy array for convenience\nX = penguins.to_numpy()\n\n# K-Means parameters\nK = 3\nnp.random.seed(42)\n# Randomly choose K unique indices for initial centroids\ninitial_idx = np.random.choice(len(X), K, replace=False)\ncentroids = X[initial_idx]\nprint(\"Initial centroids (randomly chosen):\\n\", centroids)\n\nInitial centroids (randomly chosen):\n [[ 39.5 178. ]\n [ 50.9 196. ]\n [ 42.1 195. ]]\n\n# K-Means iterative process\nmax_iters = 100\ncentroid_history = [centroids.copy()]  # store centroids at each iteration\ncluster_history = []  # store cluster assignments at each iteration\n\nfor itr in range(max_iters):\n    # Step 1: Assign points to the nearest centroid\n    distances = np.linalg.norm(\n        X[:, None] - centroids[None, :], axis=2\n    )  # distance to each centroid\n    clusters = np.argmin(distances, axis=1)  # index of nearest centroid for each point\n    cluster_history.append(clusters)\n\n    # Step 2: Update centroids to the mean of assigned points\n    new_centroids = np.array(\n        [\n            X[clusters == k].mean(axis=0) if np.any(clusters == k) else centroids[k]\n            for k in range(K)\n        ]\n    )\n    # Check for convergence (if centroids are unchanged)\n    if np.allclose(new_centroids, centroids):\n        centroids = new_centroids\n        centroid_history.append(centroids.copy())\n        print(f\"Converged after {itr} iterations.\")\n        break\n    centroids = new_centroids\n    centroid_history.append(centroids.copy())\n\nConverged after 9 iterations.\n\n# Final centroids and cluster assignment\nfinal_centroids = centroids\nfinal_clusters = cluster_history[-1]\nprint(\"Final centroids:\\n\", final_centroids)\n\nFinal centroids:\n [[ 38.45304348 187.05217391]\n [ 47.6296     216.92      ]\n [ 45.95483871 196.7311828 ]]"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects for MGTA 459",
    "section": "",
    "text": "Machine Learning\n\n\n \n\nAssignment 4\n\n\n\nJun 11, 2025\nJuan Hernández Guizar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Nomial Logit (MNL) & Conjoint\n\n\n \n\nAssignment 3\n\n\n\nMay 28, 2025\nJuan Hernández Guizar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n \n\nAssignment 2\n\n\n\nMay 7, 2025\nJuan Hernández Guizar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n \n\nAssignment 1\n\n\n\nApr 23, 2025\nJuan Hernández Guizar\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/Assignment 3/Assignment_3.html",
    "href": "blog/Assignment 3/Assignment_3.html",
    "title": "Multi-Nomial Logit (MNL) & Conjoint",
    "section": "",
    "text": "This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/Assignment 3/Assignment_3.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/Assignment 3/Assignment_3.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multi-Nomial Logit (MNL) & Conjoint",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/Assignment 3/Assignment_3.html#simulate-conjoint-data",
    "href": "blog/Assignment 3/Assignment_3.html#simulate-conjoint-data",
    "title": "Multi-Nomial Logit (MNL) & Conjoint",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nTo visualize this we developed a simulation of the conjoin data below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresp\ntask\nbrand\nad\nprice\nchoice\n\n\n\n\n0\n1\n1\nP\nNo\n32\n0\n\n\n1\n1\n1\nN\nNo\n28\n0\n\n\n2\n1\n1\nN\nNo\n24\n1\n\n\n3\n1\n2\nH\nNo\n28\n0\n\n\n4\n1\n2\nH\nNo\n8\n1\n\n\n\n\n\n\n\n\n\nThe simulated dataset contains 3000 rows and 6 columns (exactly 100 respondents × 10 tasks × 3 alternatives). Just as expected!"
  },
  {
    "objectID": "blog/Assignment 3/Assignment_3.html#preparing-the-data-for-estimation",
    "href": "blog/Assignment 3/Assignment_3.html#preparing-the-data-for-estimation",
    "title": "Multi-Nomial Logit (MNL) & Conjoint",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\nBelow, we load the conjoint dataset and reshape it for estimation. We create dummy variables for the brand and ad features (using Hulu and “No Ads” as the base levels), and we construct a task identifier to group alternatives belonging to the same choice set. We then display the first few rows to verify the structure:\n\n\n   resp  task  choice brand   ad  price  brand_N  brand_P  ad_yes  task_id\n0     1     1       1     N  Yes     28        1        0       1        0\n1     1     1       0     H  Yes     16        0        0       1        0\n2     1     1       0     P  Yes     16        0        1       1        0\n3     1     2       0     N  Yes     32        1        0       1        1\n4     1     2       1     P  Yes     16        0        1       1        1\n\n\nWe can see that each choice task (task_id) contains three rows (one per alternative). For example, resp=1, task=1 (first three rows) had alternatives from brands N, H, P all with ads (“Yes”) and different prices, and the first row (brand=N, price=28) was marked choice=1 as the selected option (it had the highest simulated utility). The dataset is now ready for model estimation."
  },
  {
    "objectID": "blog/Assignment 3/Assignment_3.html#estimation-via-maximum-likelihood",
    "href": "blog/Assignment 3/Assignment_3.html#estimation-via-maximum-likelihood",
    "title": "Multi-Nomial Logit (MNL) & Conjoint",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nWith the data prepared, we now turn to estimating the MNL model parameters by Maximum Likelihood. We have four parameters to estimate: \\(\\beta_{\\text{Netflix}}\\) and \\(\\beta_{\\text{Prime}}\\) for the two non-baseline brands (Hulu is the baseline, so its effect is 0 by construction), \\(\\beta_{\\text{Ads}}\\) for the effect of having ads (versus no ads), and \\(\\beta_{\\text{Price}}\\) for the price coefficient.\nLog-Likelihood Function: First, we need to code up the log-likelihood function for the MNL. Using the data, the log-likelihood \\(\\ell_n(\\beta)\\) can be computed by summing, over all choice tasks, the log of the probability of the chosen alternative. Given our data structure, a convenient method is: for each choice task, find the linear utility \\(v_{ij} = x_j’ \\beta\\) for each alternative, compute the choice probabilities \\(\\mathbb{P}_i(j)\\) via the softmax formula, then accumulate \\(\\log \\mathbb{P}_i(j^)\\) for the chosen alternative \\(j^{*}\\). We implement this below. To speed up computation, we vectorize the operations using NumPy:\n\nimport numpy as np\n\n# Extract numpy arrays for faster computation\nX = df[[\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"]].values  # Feature matrix (3000 x 4)\ngroup_ids = df[\"task_id\"].values  # Task identifiers (length 3000)\nchoice = df[\"choice\"].values  # Chosen indicator (0/1 for each row)\n\n\n# Define the log-likelihood function for parameters beta (as a NumPy array)\ndef loglik(beta):\n    beta = np.array(beta)\n    # Compute linear utility v = X * beta for all alternatives\n    v = X.dot(beta)  # shape (3000,)\n    # Compute exp(v) and sum exp(v) by task (denominator of softmax)\n    exp_v = np.exp(v)\n    # Sum of exp(v) for each task (using group_ids to aggregate)\n    sum_exp_v_by_task = np.bincount(\n        group_ids, weights=exp_v, minlength=df[\"task_id\"].nunique()\n    )\n    # Sum of v for the chosen alternative in each task (only one chosen per task)\n    chosen_v_by_task = np.bincount(\n        group_ids, weights=v * choice, minlength=df[\"task_id\"].nunique()\n    )\n    # Log-likelihood is sum over tasks of (v_chosen - log(sum_exp_v))\n    log_lik_value = np.sum(chosen_v_by_task - np.log(sum_exp_v_by_task))\n    return log_lik_value\n\n\n# Quick sanity check: compute log-likelihood at the true beta values used in simulation\nbeta_true = np.array([1.0, 0.5, -0.8, -0.1])\nprint(f\"Log-likelihood at true beta: {loglik(beta_true):.3f}\")\n\nLog-likelihood at true beta: -880.344\n\n\nWe included a quick check: plugging in the true part-worths [1.0, 0.5, -0.8, -0.1] gives a log-likelihood of roughly -880.3. Now, we will let the computer search for the \\(\\beta\\) that maximizes the log-likelihood. In practice, we maximize the likelihood by minimizing the negative log-likelihood. We can use a numerical optimizer (from SciPy in Python) to find the Maximum Likelihood Estimates (MLEs) of \\(\\beta\\). We also compute the Hessian-based standard errors and 95% confidence intervals for these estimates. The table below summarizes the results:\n\n\nParameter  Estimate  Std. Error 95% Confidence Interval\nβ_Netflix     0.941       0.114          [0.717, 1.165]\n  β_Prime     0.502       0.121          [0.265, 0.738]\n    β_Ads    -0.732       0.089        [-0.906, -0.558]\n  β_Price    -0.099       0.006        [-0.112, -0.087]\n\n\nAll four estimates are very close to the true values used in the simulation, which is reassuring. The estimate for β_Netflix is about 0.94 (true was 1.0) and for β_Prime about 0.50 (true 0.5), with Hulu as the baseline (so Hulu’s implicit β is 0). This means that, holding ads and price constant, a Netflix offering has about 0.94 higher utility units than an otherwise identical Hulu offering, and Prime has 0.50 higher utility than Hulu. The Ads coefficient is -0.732, indicating a strong negative effect of having advertisements: an offering with ads is less attractive by ~0.73 utility units compared to an ad-free equivalent. The Price coefficient is -0.099, meaning each additional $1 per month reduces utility by ~0.099. All parameters are significantly different from zero at the 95% confidence level (zero lies outside all the confidence intervals), aligning with our expectations (e.g., higher price and ads included both significantly reduce the likelihood of choice)."
  },
  {
    "objectID": "blog/Assignment 3/Assignment_3.html#estimation-via-bayesian-methods",
    "href": "blog/Assignment 3/Assignment_3.html#estimation-via-bayesian-methods",
    "title": "Multi-Nomial Logit (MNL) & Conjoint",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nNext, we estimate the MNL model via a Bayesian approach. Rather than finding a single best estimate as in MLE, Bayesian inference will produce a posterior distribution for the parameters. We use a Metropolis-Hastings (M-H) algorithm (a type of Markov Chain Monte Carlo) to sample from the posterior distribution of \\(\\beta\\).\nPriors: We choose relatively non-informative (weak) priors for the parameters. For the binary feature coefficients (\\(\\beta_{\\text{Netflix}}, \\beta_{\\text{Prime}}, \\beta_{\\text{Ads}}\\)) we use independent priors \\(\\mathcal{N}(0,;5)\\) – a normal distribution with mean 0 and variance 5 (standard deviation \\(\\approx 2.236\\)). For the price coefficient \\(\\beta_{\\text{Price}}\\), we use a slightly more informative prior \\(\\mathcal{N}(0,;1)\\) (mean 0, variance 1) since price effects are often tighter in range. These priors still cover a wide range of plausible values relative to our expected magnitudes, essentially adding only a gentle regularization around 0.\nMCMC Setup: We will run the M-H sampler for 11,000 iterations and discard the first 1,000 draws as “burn-in” to allow the chain to converge. This will leave us with 10,000 posterior draws for inference. We construct a symmetric proposal distribution as a multivariate normal with no covariance between parameters (diagonal covariance matrix). In particular, we use independent normal proposal steps with standard deviations: 0.05 for each of \\(\\beta_{\\text{Netflix}}, \\beta_{\\text{Prime}}, \\beta_{\\text{Ads}}\\), and 0.005 for \\(\\beta_{\\text{Price}}\\). These step sizes (chosen based on the scale of the data and priors) should yield a reasonable acceptance rate for the M-H algorithm.\nBelow is the Python implementation of the Metropolis-Hastings sampler for the posterior of \\(\\beta\\):\n\nimport math\n\n\n# Define log-posterior function (log-likelihood + log-prior)\ndef log_posterior(beta):\n    # log-likelihood from data:\n    log_lik_val = loglik(beta)\n    # log-prior for each parameter:\n    # Prior variances: 5 for Netflix/Prime/Ads, 1 for Price\n    beta = np.array(beta)\n    # Log-prior for three binary attribute betas ~ N(0,5)\n    var_bin = 5.0\n    log_prior_bin = -0.5 * (\n        3 * math.log(2 * math.pi * var_bin) + np.sum(beta[:3] ** 2) / var_bin\n    )\n    # Log-prior for price beta ~ N(0,1)\n    var_price = 1.0\n    log_prior_price = -0.5 * (\n        math.log(2 * math.pi * var_price) + (beta[3] ** 2) / var_price\n    )\n    return log_lik_val + log_prior_bin + log_prior_price\n\n\n# M-H sampling parameters\niterations = 11000\nburn_in = 1000\n\n# Proposal distribution standard deviations for [Netflix, Prime, Ads, Price]\nproposal_sd = np.array([0.05, 0.05, 0.05, 0.005])\n\n# Initialize chain and starting value (use MLE as starting point for efficiency)\nchain = np.zeros((iterations, 4))\nchain[0] = beta_hat  # start at MLE estimate (could also start at [0,0,0,0])\n\ncurrent_beta = chain[0].copy()\ncurrent_logpost = log_posterior(current_beta)\n\naccept_count = 0\nfor t in range(1, iterations):\n    # Propose a new beta by adding random normal noise to each dimension\n    proposal = current_beta + np.random.normal(0, proposal_sd, size=4)\n    prop_logpost = log_posterior(proposal)\n    # Metropolis-Hastings acceptance probability\n    log_accept_ratio = prop_logpost - current_logpost\n    if math.log(np.random.rand()) &lt; log_accept_ratio:\n        # Accept proposal\n        current_beta = proposal\n        current_logpost = prop_logpost\n        chain[t] = proposal\n        accept_count += 1\n    else:\n        # Reject proposal (retain current beta)\n        chain[t] = current_beta\n\naccept_rate = accept_count / (iterations - 1)\nprint(f\"Acceptance rate: {accept_rate:.3f}\")\n\nAcceptance rate: 0.566\n\n\nWe print the acceptance rate to ensure the sampler is moving adequately. In this run, the acceptance rate is around 50-60%, indicating the proposal step sizes are reasonable for efficient mixing (neither too high nor too low).\nAfter discarding the first 1,000 draws as burn-in, we use the remaining 10,000 posterior draws to analyze the posterior distribution of each parameter. To illustrate convergence and the posterior distribution, the following figures show the trace and the histogram of the posterior sample for one of the parameters (here we choose \\(\\beta_{\\text{Netflix}}\\) as an example):\n\n\n\n\n\n\n\n\n\nTrace plot of the MCMC chain for \\(\\beta_{\\text{Netflix}}\\) after burn-in. The chain fluctuates around its mean (red dashed line), indicating good mixing.\n\n\n\n\n\n\n\n\n\nHistogram of the posterior draws for \\(\\beta_{\\text{Netflix}}\\). The distribution is approximately normal. The red line marks the posterior mean, and the black dotted lines mark the 95% credible interval bounds.\nUsing the posterior sample, we can summarize the Bayesian estimates for all four parameters. Below we report the posterior mean, standard deviation, and 95% credible interval for each \\(\\beta\\), and compare them to the earlier MLE results: • \\(\\beta_{\\text{Netflix}}\\): Posterior mean = 0.941, SD = 0.112, 95% credible interval [0.721, 1.157]. • \\(\\beta_{\\text{Prime}}\\): Posterior mean = 0.502, SD = 0.113, 95% credible interval [0.277, 0.723]. • \\(\\beta_{\\text{Ads}}\\): Posterior mean = -0.734, SD = 0.088, 95% credible interval [-0.907, -0.566]. • \\(\\beta_{\\text{Price}}\\): Posterior mean = -0.100, SD = 0.006, 95% credible interval [-0.112, -0.088].\nThese posterior estimates are virtually identical to the MLE results we obtained earlier. The weak priors did not substantially pull the estimates toward zero, so the posterior means (0.941, 0.502, -0.734, -0.100) are almost the same as the MLE point estimates (0.941, 0.502, -0.732, -0.099). Likewise, the posterior standard deviations are in line with the MLE standard errors, and the 95% credible intervals correspond closely to the 95% confidence intervals from the likelihood approach. This consistency provides a nice validation: given our large sample (1000 choice tasks) and relatively uninformative priors, the Bayesian and frequentist approaches yield the same substantive conclusions."
  },
  {
    "objectID": "blog/Assignment 3/Assignment_3.html#discussion",
    "href": "blog/Assignment 3/Assignment_3.html#discussion",
    "title": "Multi-Nomial Logit (MNL) & Conjoint",
    "section": "6. Discussion",
    "text": "6. Discussion\nIf we consider these results as if they came from a real conjoint study (i.e. not knowing the “true” simulation values), we can interpret the parameter estimates in practical terms. We observe that \\(\\beta_{\\text{Netflix}}\\) (approximately 0.94) is larger than \\(\\beta_{\\text{Prime}}\\) (about 0.50). This indicates that, on average, consumers in the study derive more utility from the Netflix brand than from Amazon Prime, with Hulu as the baseline (zero effect). In other words, Netflix is the most preferred streaming brand among the three, and Amazon Prime is also preferred to Hulu but less so than Netflix. The negative \\(\\beta_{\\text{Ads}} \\approx -0.73\\) confirms that including advertisements substantially lowers consumer utility compared to an ad-free experience. Similarly, \\(\\beta_{\\text{Price}}\\) is about -0.10, meaning higher price has a negative effect on choice probability (which makes intuitive sense – consumers prefer cheaper options, all else equal).\n\nIt is important to note that \\(\\beta_{\\text{Price}}\\) being negative is not only sensible, but we can also quantify its meaning: a one-unit increase in utility is equivalent to about $10 in price (since \\(1/0.10 = 10\\)). Therefore, the brand coefficient for Netflix (0.94) can be interpreted as Netflix providing roughly $9–$10 worth of added value to consumers relative to Hulu, and Prime’s brand value is about $5 relative to Hulu. Likewise, having ads (\\(\\beta_{\\text{Ads}} \\approx -0.73\\)) imposes a disutility roughly equivalent to $7–$8 per month in price. These insights are very useful for business decisions – for instance, they indicate how much more a company could potentially charge for a Netflix-branded service (or how much discount would be required to compensate for including ads).\nFinally, how would we extend this to a multi-level (random-parameter) model? In a multi-level or hierarchical Bayesian conjoint model, we acknowledge that different consumers may have different preference parameters. Instead of one \\(\\beta\\) vector for the whole population, we assume each individual \\(i\\) has their own \\(\\beta_i\\), drawn from a population distribution (for example, \\(\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\\)). To simulate data from such a model, we would first draw each respondent’s true part-worths from a specified distribution (with some mean and variance to represent preference heterogeneity), and then use those individual-level \\(\\beta_i\\) to simulate each person’s choices. To estimate a hierarchical model, we would need to introduce additional layers in our estimation procedure – essentially estimating both the individual-level \\(\\beta_i\\) for each respondent and the hyperparameters (like the mean vector \\(\\mu\\) and covariance \\(\\Sigma\\) of the population distribution). This could be done via hierarchical Bayesian methods (where we would add Gibbs sampling or more complex MCMC steps to sample each \\(\\beta_i\\) and the hyperparameters) or via simulated maximum likelihood (also known as a mixed logit approach). In essence, the key change is moving from a fixed-effects MNL (one common \\(\\beta\\) for all) to a random-effects MNL where each consumer has their own \\(\\beta\\) drawn from a higher-level distribution. This addition would capture the real-world preference heterogeneity we expect in conjoint analysis, at the cost of a more complex simulation and estimation process.\nIn this blog, we successfully implemented and compared maximum likelihood and Bayesian estimation for a multinomial logit conjoint model. Both approaches recovered the true part-worth parameters closely, reinforcing our understanding of MNL. From a business perspective, the results highlight clear patterns: brand matters (Netflix holds a strong advantage over competitors), ads are detrimental to user utility, and price sensitivity is significant. These findings imply that a streaming service can command a premium for a stronger brand or no-ad experience, whereas introducing ads or raising prices must be balanced against the substantial utility loss. By quantifying these trade-offs, conjoint analysis provides valuable guidance for product design and pricing strategy in the streaming industry."
  },
  {
    "objectID": "blog/Assignment 2/Assignment_2.html",
    "href": "blog/Assignment 2/Assignment_2.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nTo start we will review the first 5 data points collected in a table format:\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nBelow is the distribution of patents among Blueprinty customers vs non-customers. The histogram shows that Blueprinty customers (light orange bars) tend to have more patents on average than non-customers (light blue bars).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn average, Blueprinty customers have approximately 4.13 patents over 5 years, compared to about 3.47 for non-customers. While this naive comparison suggests customers produce more patents, we must consider that Blueprinty’s customers may differ systematically in other ways (e.g. perhaps they are older firms or clustered in certain regions).\nLet’s examine the age and regional composition of customers vs non-customers.\n\n\n\n\n\n\n\n\n\nBlueprinty customers have a slightly higher mean age since incorporation (about 26.9 years) than non-customers (26.1 years), but the age distributions largely overlap (both groups are typically 20–30 years old, with only minor differences). This suggests that firm age might not differ dramatically by customer status, though we will account for age in the analysis.\nRegionally, there are stark differences in who adopts Blueprinty’s software.\n\n\n\n\n\n\n\n\n\nCounts of firms by region and Blueprinty customer status. In the Northeast region, the green bar (Blueprinty customers) is higher than the blue bar (non-customers), indicating a large share of Blueprinty’s users are in the Northeast. In contrast, in all other regions (Midwest, South, Southwest, Northwest) the majority of firms are non-customers. This reveals that Blueprinty’s customer base is heavily concentrated in the Northeast, which suggests potential selection bias by region.\nIndeed, about 68% of Blueprinty’s customers are located in the Northeast, whereas only ~27% of non-customer firms are in the Northeast. Other regions (Midwest, South, Southwest, Northwest) are under-represented among customers relative to non-customers. This imbalance means any raw difference in patent counts could partly reflect regional effects. In summary, Blueprinty customers tend to have slightly older firms (though age differences are minor) and are much more likely to be in the Northeast region. We will need to control for these factors when analyzing the effect of Blueprinty’s software on patent output.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nBelow, \\(Y_i\\) is the patent count for firm \\(i\\) and \\(\\lambda\\) is the average number of patents per firm in five years.\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(Y_i\\)\nObserved patent count for firm \\(i\\) (integer \\(\\ge 0\\))\n\n\n\\(n\\)\nTotal number of firms (length of \\(\\mathbf y\\))\n\n\n\\(\\lambda\\)\nPoisson rate parameter — the mean (and variance) of the distribution\n\n\n\\(\\mathbf y\\)\nColumn vector of all counts: \\(\\mathbf y = (\\,y_{1},\\,y_{2},\\,\\dots,\\,y_{n})^{\\!\\top}\\)\n\n\n\\(\\mathcal L(\\lambda;\\mathbf y)\\)\nLikelihood of the entire dataset, given \\(\\lambda\\)\n\n\n\\(\\ell(\\lambda)\\)\nLog-likelihood, \\(\\ell(\\lambda)=\\log \\mathcal L(\\lambda;\\mathbf y)\\)\n\n\n\n\\[\nP\\!\\bigl(Y_i=y_i \\mid \\lambda\\bigr)=\n  \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!},\n  \\qquad y_i=0,1,2,\\dots\n\\]\n\\[\n\\boxed{\n  \\mathcal L(\\lambda;\\mathbf y)=\n    e^{-n\\lambda}\\,\n    \\lambda^{\\sum_{i=1}^{n} y_i}\\!\n    \\Big/\n    \\prod_{i=1}^{n} y_i!\n}\n\\qquad\n\\boxed{\n  \\ell(\\lambda)=\n    \\sum_{i=1}^{n}\\!\n      \\bigl(\n        y_i\\log\\lambda-\\lambda-\\log y_i!\n      \\bigr)\n}\n\\]\nPutting all counts into one vector lets us write the likelihood compactly and pass the entire dataset to a single log-likelihood function. In the code below that function is called loglik_poisson. It follows the “parameter-vector” convention most optimisers expect: the one unknown, lambda, is stored as theta[0]. This style makes the function future-proof—if we later add more parameters we can just extend the theta vector without rewriting the optimiser call.\n\nimport numpy as np, math\n\n\ndef loglik_poisson(theta, y):\n    \"\"\"\n    Poisson log-likelihood\n\n    Parameters\n    ----------\n    theta : 1-element array-like\n        theta[0] = λ  (must be &gt; 0)\n    y     : 1-D numpy array of non-negative integers\n\n    Returns\n    -------\n    float\n        Scalar log-likelihood ℓ(λ)\n    \"\"\"\n    lam = float(theta[0])  # ← parallels 'mu &lt;- theta[1]'\n    if lam &lt;= 0:\n        return -np.inf  # guard just like s2&gt;0 in Normal case\n\n    n = y.size\n    # ll = Σ(y_i log λ) − n λ − Σ log(y_i!)\n    ll = np.sum(y * np.log(lam)) - n * lam - np.sum([math.lgamma(k + 1) for k in y])\n    return ll\n\nThe curve below shows how the log-likelihood changes as we slide λ across plausible values. It rises steeply, flattens, and then falls—peaking (unsurprisingly) right where λ equals the sample mean (~3.7 patents). That single highest point is the Maximum-Likelihood Estimate: the value of λ that makes the observed patent counts most probable under a Poisson model.\n\n\n\n\n\n\n\n\n\n\\[\nP\\!\\bigl(Y_i = y_i \\mid \\lambda\\bigr)=\n  \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!},\n  \\qquad y_i = 0,1,2,\\dots\n\\]\nDifferentiating the log-likelihood\n\\[\n\\ell(\\lambda)=\\sum_{i=1}^{n}\n  \\bigl(\n    y_i\\log\\lambda-\\lambda-\\log y_i!\n  \\bigr)\n\\]\nwith respect to () and setting the derivative to zero gives\n\\[\n\\frac{\\partial \\ell}{\\partial \\lambda}\n  \\;=\\;\n  \\frac{\\sum_{i=1}^{n}y_i}{\\lambda}-n\n  \\;=\\;0\n  \\;\\Longrightarrow\\;\n  \\boxed{\\hat\\lambda=\\bar y}\n\\]\nso the maximum-likelihood estimate is nothing more than the sample mean of the counts. The first code cell reflects that algebra exactly: y.mean() is computed and printed as the Analytic MLE, which for our data equals 3.6847 patents per firm.\n\n\nAnalytic MLE   λ̂ = 3.6847\n\n\n\n\nOptimiser MLE  λ̂ = 3.6847\n\n\nThe second cell tackles the same task numerically. scipy.optimize.minimize_scalar is instructed to minimise the negative log-likelihood (equivalently maximise ()), searching over the interval ([10^{-4},10]). Because the optimiser treats () as a scalar, we wrap it in a one-element list when passing it to loglik_poisson. After a quick line search it returns an Optimiser MLE of 3.6847, matching the analytic result to four decimal places—strong confirmation that the calculus and the numerical optimisation tell the same story.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\nA covariate (sometimes called a feature or explanatory variable) is simply an observed attribute we believe helps explain the outcome. Here our covariates are age, age ² (to capture curvature), a set of region dummies, and a binary flag for Blueprinty customer status. By stacking these in a matrix \\(X\\) and multiplying by a coefficient vector \\(\\boldsymbol\\beta\\), we let each firm have its own mean rate \\(\\lambda_i=\\exp(X_i^{!\\top}\\boldsymbol\\beta)\\)—the exponential ensures every \\(\\lambda_i\\) stays positive.\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(Y_i\\)\nObserved patent count for firm \\(i\\) (integer \\(\\ge 0\\))\n\n\n\\(X_i\\)\nRow vector of covariates for firm \\(i\\) (intercept, age, age\\(^{2}\\), region dummies, Blueprinty flag)\n\n\n\\(\\boldsymbol\\beta\\)\nColumn vector of coefficients (one per covariate)\n\n\n\\(\\lambda_i\\)\nMean patents for firm \\(i\\): \\(\\lambda_i = \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\\)\n\n\n\\(n\\)\nTotal number of firms (rows of \\(X\\))\n\n\n\\(\\mathbf y\\)\nColumn vector of all counts: \\(\\mathbf y = (\\,y_{1},\\,y_{2},\\,\\dots,\\,y_{n})^{\\!\\top}\\)\n\n\n\\(X\\)\nDesign matrix that stacks all \\(X_i\\) rows\n\n\n\\(\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\\)\nLikelihood of the entire dataset, given \\(\\boldsymbol\\beta\\)\n\n\n\\(\\ell(\\boldsymbol\\beta)\\)\nLog-likelihood, \\(\\ell(\\boldsymbol\\beta)=\\log \\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\\)\n\n\n\nExpanding from a constant‐rate model to Poisson regression swaps the single parameter \\(\\lambda\\) for a whole vector of coefficients \\(\\boldsymbol\\beta\\).\nEach firm now gets its own mean rate through the inverse-link function\n\\(\\lambda_i=\\exp(X_i^{\\!\\top}\\boldsymbol\\beta)\\), guaranteeing positivity while letting the linear predictor \\(X_i^{\\!\\top}\\boldsymbol\\beta\\) wander over the real line.\nThe covariate matrix \\(X\\) holds an intercept, age, age², a set of region dummies, and a Blueprinty-customer flag, so any of those characteristics can nudge the expected patent count up or down.\n\\[\nY_i \\,\\bigl|\\, X_i \\;\\sim\\; \\operatorname{Poisson}\\!\\bigl(\\lambda_i\\bigr),\n\\qquad\n\\lambda_i \\;=\\; \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr),\n\\qquad i = 1,\\dots,n.\n\\]\n\\[\n\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\n  \\;=\\;\n  \\prod_{i=1}^{n}\n    \\frac{e^{-\\lambda_i}\\,\\lambda_i^{\\,Y_i}}{Y_i!},\n\\qquad\n\\ell(\\boldsymbol\\beta)\n  \\;=\\;\n  \\sum_{i=1}^{n}\n    \\Bigl(\n      Y_i\\,X_i^{\\!\\top}\\boldsymbol\\beta\n      \\;-\\;\n      \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\n      \\;-\\;\n      \\log Y_i!\n    \\Bigr).\n\\]\nThe code block that follows translates this math into Python.\nloglik_poisson_reg(beta, y, X) now takes both the response vector and the covariate matrix, computes the linear predictor \\(X\\boldsymbol\\beta\\), exponentiates to obtain \\(\\boldsymbol\\lambda\\), and returns the scalar log-likelihood. Passing that function to an optimiser (e.g. scipy.optimize.minimize) yields the maximum-likelihood estimates for the full coefficient vector \\(\\boldsymbol\\beta\\).\n\nimport numpy as np, math\n\n\ndef loglik_poisson_reg(beta, y, X):\n    \"\"\"\n    Poisson regression log-likelihood.\n\n    beta : 1-D array, length p           (coefficients)\n    y    : 1-D array, length n           (counts)\n    X    : 2-D array, shape (n, p)       (covariate matrix)\n\n    Returns\n    -------\n    float : scalar log-likelihood ℓ(β)\n    \"\"\"\n    eta = X @ beta  # linear predictor  η = Xβ  (shape n)\n    lam = np.exp(eta)  # inverse link  λ = exp(η)\n    if np.any(lam &lt;= 0):\n        return -np.inf  # numerical safety\n    ll = np.sum(y * eta - lam - [math.lgamma(k + 1) for k in y])\n    return ll\n\nWith this function we can now hand the entire β vector to an optimiser (e.g. scipy.optimize.minimize) to obtain maximum-likelihood estimates, just as we did for the single-parameter case—only now the model flexes with age, geography, and Blueprinty adoption.\n\\[\n\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\n  = \\prod_{i=1}^{n}\n      \\frac{e^{-\\lambda_i}\\lambda_i^{\\,Y_i}}{Y_i!},\n\\qquad\n\\ell(\\boldsymbol\\beta)\n  = \\sum_{i=1}^{n}\n      \\Bigl(\n        Y_i\\,X_i^{\\!\\top}\\boldsymbol\\beta\n        - \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\n        - \\log Y_i!\n      \\Bigr).\n\\]\nA Hessian is the log-likelihood’s curvature map. Picture the likelihood surface as a hill; the Hessian tells us how sharply that hill drops away in every parameter direction. Formally\n\\[\nH(\\hat{\\boldsymbol\\beta})\n  = -\n    \\frac{\\partial^2\\ell(\\boldsymbol\\beta)}\n         {\\partial\\boldsymbol\\beta\\,\\partial\\boldsymbol\\beta^{\\!\\top}}\n  \\Biggr\\rvert_{\\;\\boldsymbol\\beta=\\hat{\\boldsymbol\\beta}},\n\\]\nand its negative inverse is the large-sample covariance of the MLEs, so ((_j)=).\n\n\n\n\n\n\n\n\n\nCoefficient\nStd.Error\n\n\n\n\nconst\n-0.5089\n0.1870\n\n\nage\n0.1486\n0.0140\n\n\nage2\n-0.2970\n0.0260\n\n\niscustomer\n0.2076\n0.0330\n\n\nregion_Northeast\n0.0292\n0.0465\n\n\nregion_Northwest\n-0.0176\n0.0572\n\n\nregion_South\n0.0566\n0.0558\n\n\nregion_Southwest\n0.0506\n0.0501\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nMon, 09 Jun 2025\nDeviance:\n2143.3\n\n\nTime:\n20:37:06\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nx1\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nx2\n-0.2970\n0.026\n-11.513\n0.000\n-0.348\n-0.246\n\n\nx3\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nx4\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nx5\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nx6\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nx7\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\nAfter rescaling age² and bounding the search, the Poisson regression converges cleanly. Key take-aways:\n\nAge (+) and Age² (−) form a concave pattern—patenting rises, peaks mid-20s, then tapers.\nRegion dummies shrink toward zero once we explicitly control for Blueprinty usage, implying geography itself isn’t the driver; the earlier Northeast spike simply reflected the high concentration of customers there.\nBlueprinty customer (+0.208) yields ≈ 23 % higher expected patents, highly significant even after all other controls.\n\nHand-rolled MLEs, Hessian-based standard errors, and statsmodels GLM all agree, giving us confidence in the estimates and the narrative.\nTo translate the log-coefficient into “extra patents,” we ran a simple counter-factual:\n\nX₀: keep every firm’s age and region but set iscustomer = 0.\n\nX₁: identical matrix but flip iscustomer = 1.\n\nPredict \\(\\hat y_0=\\exp(X_0\\hat\\beta)\\) and \\(\\hat y_1=\\exp(X_1\\hat\\beta)\\).\n\nTake the firm-by-firm difference \\(\\hat y_1-\\hat y_0\\) and average.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-customer\nBlueprinty\nLift\n\n\n\n\n0\n3.44\n4.23\n0.79\n\n\n\n\n\n\n\nThe result: +0.82 patents per firm over five years, about a 22 % lift relative to the baseline mean. The density plot below shows most firms gain between 0.5 and 1.1 extra patents, with a long but light right tail for the largest firms.\n\n\n\nAfter accounting for firm age and regional differences, using Blueprinty still delivers about one additional granted patent every five years. For most engineering shops that’s a solid, tangible boost—enough to nudge a “nice idea” into a fully protected asset on the balance sheet."
  },
  {
    "objectID": "blog/Assignment 2/Assignment_2.html#blueprinty-case-study",
    "href": "blog/Assignment 2/Assignment_2.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nTo start we will review the first 5 data points collected in a table format:\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nBelow is the distribution of patents among Blueprinty customers vs non-customers. The histogram shows that Blueprinty customers (light orange bars) tend to have more patents on average than non-customers (light blue bars).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn average, Blueprinty customers have approximately 4.13 patents over 5 years, compared to about 3.47 for non-customers. While this naive comparison suggests customers produce more patents, we must consider that Blueprinty’s customers may differ systematically in other ways (e.g. perhaps they are older firms or clustered in certain regions).\nLet’s examine the age and regional composition of customers vs non-customers.\n\n\n\n\n\n\n\n\n\nBlueprinty customers have a slightly higher mean age since incorporation (about 26.9 years) than non-customers (26.1 years), but the age distributions largely overlap (both groups are typically 20–30 years old, with only minor differences). This suggests that firm age might not differ dramatically by customer status, though we will account for age in the analysis.\nRegionally, there are stark differences in who adopts Blueprinty’s software.\n\n\n\n\n\n\n\n\n\nCounts of firms by region and Blueprinty customer status. In the Northeast region, the green bar (Blueprinty customers) is higher than the blue bar (non-customers), indicating a large share of Blueprinty’s users are in the Northeast. In contrast, in all other regions (Midwest, South, Southwest, Northwest) the majority of firms are non-customers. This reveals that Blueprinty’s customer base is heavily concentrated in the Northeast, which suggests potential selection bias by region.\nIndeed, about 68% of Blueprinty’s customers are located in the Northeast, whereas only ~27% of non-customer firms are in the Northeast. Other regions (Midwest, South, Southwest, Northwest) are under-represented among customers relative to non-customers. This imbalance means any raw difference in patent counts could partly reflect regional effects. In summary, Blueprinty customers tend to have slightly older firms (though age differences are minor) and are much more likely to be in the Northeast region. We will need to control for these factors when analyzing the effect of Blueprinty’s software on patent output.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nBelow, \\(Y_i\\) is the patent count for firm \\(i\\) and \\(\\lambda\\) is the average number of patents per firm in five years.\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(Y_i\\)\nObserved patent count for firm \\(i\\) (integer \\(\\ge 0\\))\n\n\n\\(n\\)\nTotal number of firms (length of \\(\\mathbf y\\))\n\n\n\\(\\lambda\\)\nPoisson rate parameter — the mean (and variance) of the distribution\n\n\n\\(\\mathbf y\\)\nColumn vector of all counts: \\(\\mathbf y = (\\,y_{1},\\,y_{2},\\,\\dots,\\,y_{n})^{\\!\\top}\\)\n\n\n\\(\\mathcal L(\\lambda;\\mathbf y)\\)\nLikelihood of the entire dataset, given \\(\\lambda\\)\n\n\n\\(\\ell(\\lambda)\\)\nLog-likelihood, \\(\\ell(\\lambda)=\\log \\mathcal L(\\lambda;\\mathbf y)\\)\n\n\n\n\\[\nP\\!\\bigl(Y_i=y_i \\mid \\lambda\\bigr)=\n  \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!},\n  \\qquad y_i=0,1,2,\\dots\n\\]\n\\[\n\\boxed{\n  \\mathcal L(\\lambda;\\mathbf y)=\n    e^{-n\\lambda}\\,\n    \\lambda^{\\sum_{i=1}^{n} y_i}\\!\n    \\Big/\n    \\prod_{i=1}^{n} y_i!\n}\n\\qquad\n\\boxed{\n  \\ell(\\lambda)=\n    \\sum_{i=1}^{n}\\!\n      \\bigl(\n        y_i\\log\\lambda-\\lambda-\\log y_i!\n      \\bigr)\n}\n\\]\nPutting all counts into one vector lets us write the likelihood compactly and pass the entire dataset to a single log-likelihood function. In the code below that function is called loglik_poisson. It follows the “parameter-vector” convention most optimisers expect: the one unknown, lambda, is stored as theta[0]. This style makes the function future-proof—if we later add more parameters we can just extend the theta vector without rewriting the optimiser call.\n\nimport numpy as np, math\n\n\ndef loglik_poisson(theta, y):\n    \"\"\"\n    Poisson log-likelihood\n\n    Parameters\n    ----------\n    theta : 1-element array-like\n        theta[0] = λ  (must be &gt; 0)\n    y     : 1-D numpy array of non-negative integers\n\n    Returns\n    -------\n    float\n        Scalar log-likelihood ℓ(λ)\n    \"\"\"\n    lam = float(theta[0])  # ← parallels 'mu &lt;- theta[1]'\n    if lam &lt;= 0:\n        return -np.inf  # guard just like s2&gt;0 in Normal case\n\n    n = y.size\n    # ll = Σ(y_i log λ) − n λ − Σ log(y_i!)\n    ll = np.sum(y * np.log(lam)) - n * lam - np.sum([math.lgamma(k + 1) for k in y])\n    return ll\n\nThe curve below shows how the log-likelihood changes as we slide λ across plausible values. It rises steeply, flattens, and then falls—peaking (unsurprisingly) right where λ equals the sample mean (~3.7 patents). That single highest point is the Maximum-Likelihood Estimate: the value of λ that makes the observed patent counts most probable under a Poisson model.\n\n\n\n\n\n\n\n\n\n\\[\nP\\!\\bigl(Y_i = y_i \\mid \\lambda\\bigr)=\n  \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!},\n  \\qquad y_i = 0,1,2,\\dots\n\\]\nDifferentiating the log-likelihood\n\\[\n\\ell(\\lambda)=\\sum_{i=1}^{n}\n  \\bigl(\n    y_i\\log\\lambda-\\lambda-\\log y_i!\n  \\bigr)\n\\]\nwith respect to () and setting the derivative to zero gives\n\\[\n\\frac{\\partial \\ell}{\\partial \\lambda}\n  \\;=\\;\n  \\frac{\\sum_{i=1}^{n}y_i}{\\lambda}-n\n  \\;=\\;0\n  \\;\\Longrightarrow\\;\n  \\boxed{\\hat\\lambda=\\bar y}\n\\]\nso the maximum-likelihood estimate is nothing more than the sample mean of the counts. The first code cell reflects that algebra exactly: y.mean() is computed and printed as the Analytic MLE, which for our data equals 3.6847 patents per firm.\n\n\nAnalytic MLE   λ̂ = 3.6847\n\n\n\n\nOptimiser MLE  λ̂ = 3.6847\n\n\nThe second cell tackles the same task numerically. scipy.optimize.minimize_scalar is instructed to minimise the negative log-likelihood (equivalently maximise ()), searching over the interval ([10^{-4},10]). Because the optimiser treats () as a scalar, we wrap it in a one-element list when passing it to loglik_poisson. After a quick line search it returns an Optimiser MLE of 3.6847, matching the analytic result to four decimal places—strong confirmation that the calculus and the numerical optimisation tell the same story.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\nA covariate (sometimes called a feature or explanatory variable) is simply an observed attribute we believe helps explain the outcome. Here our covariates are age, age ² (to capture curvature), a set of region dummies, and a binary flag for Blueprinty customer status. By stacking these in a matrix \\(X\\) and multiplying by a coefficient vector \\(\\boldsymbol\\beta\\), we let each firm have its own mean rate \\(\\lambda_i=\\exp(X_i^{!\\top}\\boldsymbol\\beta)\\)—the exponential ensures every \\(\\lambda_i\\) stays positive.\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(Y_i\\)\nObserved patent count for firm \\(i\\) (integer \\(\\ge 0\\))\n\n\n\\(X_i\\)\nRow vector of covariates for firm \\(i\\) (intercept, age, age\\(^{2}\\), region dummies, Blueprinty flag)\n\n\n\\(\\boldsymbol\\beta\\)\nColumn vector of coefficients (one per covariate)\n\n\n\\(\\lambda_i\\)\nMean patents for firm \\(i\\): \\(\\lambda_i = \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\\)\n\n\n\\(n\\)\nTotal number of firms (rows of \\(X\\))\n\n\n\\(\\mathbf y\\)\nColumn vector of all counts: \\(\\mathbf y = (\\,y_{1},\\,y_{2},\\,\\dots,\\,y_{n})^{\\!\\top}\\)\n\n\n\\(X\\)\nDesign matrix that stacks all \\(X_i\\) rows\n\n\n\\(\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\\)\nLikelihood of the entire dataset, given \\(\\boldsymbol\\beta\\)\n\n\n\\(\\ell(\\boldsymbol\\beta)\\)\nLog-likelihood, \\(\\ell(\\boldsymbol\\beta)=\\log \\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\\)\n\n\n\nExpanding from a constant‐rate model to Poisson regression swaps the single parameter \\(\\lambda\\) for a whole vector of coefficients \\(\\boldsymbol\\beta\\).\nEach firm now gets its own mean rate through the inverse-link function\n\\(\\lambda_i=\\exp(X_i^{\\!\\top}\\boldsymbol\\beta)\\), guaranteeing positivity while letting the linear predictor \\(X_i^{\\!\\top}\\boldsymbol\\beta\\) wander over the real line.\nThe covariate matrix \\(X\\) holds an intercept, age, age², a set of region dummies, and a Blueprinty-customer flag, so any of those characteristics can nudge the expected patent count up or down.\n\\[\nY_i \\,\\bigl|\\, X_i \\;\\sim\\; \\operatorname{Poisson}\\!\\bigl(\\lambda_i\\bigr),\n\\qquad\n\\lambda_i \\;=\\; \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr),\n\\qquad i = 1,\\dots,n.\n\\]\n\\[\n\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\n  \\;=\\;\n  \\prod_{i=1}^{n}\n    \\frac{e^{-\\lambda_i}\\,\\lambda_i^{\\,Y_i}}{Y_i!},\n\\qquad\n\\ell(\\boldsymbol\\beta)\n  \\;=\\;\n  \\sum_{i=1}^{n}\n    \\Bigl(\n      Y_i\\,X_i^{\\!\\top}\\boldsymbol\\beta\n      \\;-\\;\n      \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\n      \\;-\\;\n      \\log Y_i!\n    \\Bigr).\n\\]\nThe code block that follows translates this math into Python.\nloglik_poisson_reg(beta, y, X) now takes both the response vector and the covariate matrix, computes the linear predictor \\(X\\boldsymbol\\beta\\), exponentiates to obtain \\(\\boldsymbol\\lambda\\), and returns the scalar log-likelihood. Passing that function to an optimiser (e.g. scipy.optimize.minimize) yields the maximum-likelihood estimates for the full coefficient vector \\(\\boldsymbol\\beta\\).\n\nimport numpy as np, math\n\n\ndef loglik_poisson_reg(beta, y, X):\n    \"\"\"\n    Poisson regression log-likelihood.\n\n    beta : 1-D array, length p           (coefficients)\n    y    : 1-D array, length n           (counts)\n    X    : 2-D array, shape (n, p)       (covariate matrix)\n\n    Returns\n    -------\n    float : scalar log-likelihood ℓ(β)\n    \"\"\"\n    eta = X @ beta  # linear predictor  η = Xβ  (shape n)\n    lam = np.exp(eta)  # inverse link  λ = exp(η)\n    if np.any(lam &lt;= 0):\n        return -np.inf  # numerical safety\n    ll = np.sum(y * eta - lam - [math.lgamma(k + 1) for k in y])\n    return ll\n\nWith this function we can now hand the entire β vector to an optimiser (e.g. scipy.optimize.minimize) to obtain maximum-likelihood estimates, just as we did for the single-parameter case—only now the model flexes with age, geography, and Blueprinty adoption.\n\\[\n\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\n  = \\prod_{i=1}^{n}\n      \\frac{e^{-\\lambda_i}\\lambda_i^{\\,Y_i}}{Y_i!},\n\\qquad\n\\ell(\\boldsymbol\\beta)\n  = \\sum_{i=1}^{n}\n      \\Bigl(\n        Y_i\\,X_i^{\\!\\top}\\boldsymbol\\beta\n        - \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\n        - \\log Y_i!\n      \\Bigr).\n\\]\nA Hessian is the log-likelihood’s curvature map. Picture the likelihood surface as a hill; the Hessian tells us how sharply that hill drops away in every parameter direction. Formally\n\\[\nH(\\hat{\\boldsymbol\\beta})\n  = -\n    \\frac{\\partial^2\\ell(\\boldsymbol\\beta)}\n         {\\partial\\boldsymbol\\beta\\,\\partial\\boldsymbol\\beta^{\\!\\top}}\n  \\Biggr\\rvert_{\\;\\boldsymbol\\beta=\\hat{\\boldsymbol\\beta}},\n\\]\nand its negative inverse is the large-sample covariance of the MLEs, so ((_j)=).\n\n\n\n\n\n\n\n\n\nCoefficient\nStd.Error\n\n\n\n\nconst\n-0.5089\n0.1870\n\n\nage\n0.1486\n0.0140\n\n\nage2\n-0.2970\n0.0260\n\n\niscustomer\n0.2076\n0.0330\n\n\nregion_Northeast\n0.0292\n0.0465\n\n\nregion_Northwest\n-0.0176\n0.0572\n\n\nregion_South\n0.0566\n0.0558\n\n\nregion_Southwest\n0.0506\n0.0501\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nMon, 09 Jun 2025\nDeviance:\n2143.3\n\n\nTime:\n20:37:06\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nx1\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nx2\n-0.2970\n0.026\n-11.513\n0.000\n-0.348\n-0.246\n\n\nx3\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nx4\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nx5\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nx6\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nx7\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\nAfter rescaling age² and bounding the search, the Poisson regression converges cleanly. Key take-aways:\n\nAge (+) and Age² (−) form a concave pattern—patenting rises, peaks mid-20s, then tapers.\nRegion dummies shrink toward zero once we explicitly control for Blueprinty usage, implying geography itself isn’t the driver; the earlier Northeast spike simply reflected the high concentration of customers there.\nBlueprinty customer (+0.208) yields ≈ 23 % higher expected patents, highly significant even after all other controls.\n\nHand-rolled MLEs, Hessian-based standard errors, and statsmodels GLM all agree, giving us confidence in the estimates and the narrative.\nTo translate the log-coefficient into “extra patents,” we ran a simple counter-factual:\n\nX₀: keep every firm’s age and region but set iscustomer = 0.\n\nX₁: identical matrix but flip iscustomer = 1.\n\nPredict \\(\\hat y_0=\\exp(X_0\\hat\\beta)\\) and \\(\\hat y_1=\\exp(X_1\\hat\\beta)\\).\n\nTake the firm-by-firm difference \\(\\hat y_1-\\hat y_0\\) and average.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-customer\nBlueprinty\nLift\n\n\n\n\n0\n3.44\n4.23\n0.79\n\n\n\n\n\n\n\nThe result: +0.82 patents per firm over five years, about a 22 % lift relative to the baseline mean. The density plot below shows most firms gain between 0.5 and 1.1 extra patents, with a long but light right tail for the largest firms.\n\n\n\nAfter accounting for firm age and regional differences, using Blueprinty still delivers about one additional granted patent every five years. For most engineering shops that’s a solid, tangible boost—enough to nudge a “nice idea” into a fully protected asset on the balance sheet."
  },
  {
    "objectID": "blog/Assignment 2/Assignment_2.html#airbnb-case-study",
    "href": "blog/Assignment 2/Assignment_2.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nWe treat number of reviews as a stand-in for bookings and begin by exploring the 40,628-listing Airbnb-NYC dataset (features include listing age days, room type, bedrooms, bathrooms, nightly price, review scores for cleanliness / location / value, and an instant-bookable flag).\n\nHandling missing values – 76 listings lack bedrooms, 160 lack bathrooms, and about 10,200 lack all three review-score variables.\nMost of those 10 k are listings with zero reviews (9,481 rows, ≈ 23 % of the data).\nWe drop any row with a missing predictor to keep modeling simple, which chiefly removes those zero-review listings and leaves 30,160 listings (all with ≥ 1 review).\nShould we keep the zero-review rows?\nIncluding them would preserve information on brand-new hosts but requires imputing their absent review scores or using a two-part model.\nFor this tutorial we exclude them, accepting a bit of bias in exchange for cleaner predictors; we flag that trade-off for future work.\nFeature transformations –\ninstant_bookable is recoded from 't'/'f' to 0 / 1.\nNightly price is extremely right-skewed, so we model log_price instead, which stabilises variance and gives a roughly bell-shaped histogram.\ndays remains in raw units (median ≈ 3 years; one outlier appears at 117 years!), and no further transforms are applied at this stage.\n\nNext, let’s inspect the distribution of our key variables:\n\n\n\n\n\n\n\n\n\nLeft plot: Distribution of the number of reviews per listing (for listings with ≥1 review). The histogram is extremely right-skewed. A large fraction of listings have only a handful of reviews – for example, the median is 8 reviews, and 75% have ≤26 reviews. A long tail of popular listings have many more reviews (the maximum in this subset is 421). This heavy-tailed count distribution suggests that modeling approaches for count data (like Poisson regression) or a log-transformation may be appropriate. Note: ~23% of listings had 0 reviews (not shown here, as they were dropped for modeling), indicating many very new or less-booked listings.\nRight plot: Distribution of nightly price, in USD (left), and distribution of log-transformed price (right) for NYC Airbnb listings. The raw price distribution is highly skewed with most listings in the $50–$200 range and a few extreme outliers (up to $10,000). We limited the x-axis to $500 in the left plot for clarity, but even within this range the mass is concentrated at lower prices. The log-scale (natural log) of price, shown on the right, is much more symmetric and bell-shaped. This confirms that a log transformation of price will likely make modeling easier: a unit change in log_price corresponds to a multiplicative change in actual price, and we expect a more linear relationship with outcome variables on that scale.\nWith the data cleaned and initial insights gathered, we proceed to model the number of reviews (as a proxy for bookings) using two approaches: Poisson regression for count data, and linear regression. The response variable will be the count of reviews. In the linear model, we will use a log transformation of reviews to account for skewness, whereas the Poisson model will use the count directly with a log link function.\n\n\nPoisson Model\nA log link makes each coefficient a multiplicative bump.\n\\[\n\\ell(\\beta)=\\sum_i\\Bigl(y_iX_i’\\beta-\\exp(X_i’\\beta)-\\ln(y_i!)\\Bigr)\n\\]\n\n\n================================================================================================\n                                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------------------\nIntercept                        3.0166      0.019    156.966      0.000       2.979       3.054\nC(room_type)[T.Private room]     0.0874      0.003     25.854      0.000       0.081       0.094\nC(room_type)[T.Shared room]     -0.1033      0.009    -11.345      0.000      -0.121      -0.085\ndays                          5.056e-05   3.93e-07    128.807      0.000    4.98e-05    5.13e-05\nbedrooms                         0.0464      0.002     22.733      0.000       0.042       0.050\nbathrooms                       -0.1453      0.004    -38.805      0.000      -0.153      -0.138\nlog_price                        0.1309      0.003     45.347      0.000       0.125       0.137\nreview_scores_cleanliness        0.1088      0.001     72.531      0.000       0.106       0.112\nreview_scores_location          -0.0975      0.002    -58.982      0.000      -0.101      -0.094\nreview_scores_value             -0.0794      0.002    -43.513      0.000      -0.083      -0.076\ninstant                          0.3521      0.003    121.730      0.000       0.346       0.358\n================================================================================================\n\nIncidence-rate ratios (IRR)\n Intercept                       20.42\nC(room_type)[T.Private room]     1.09\nC(room_type)[T.Shared room]      0.90\ndays                             1.00\nbedrooms                         1.05\nbathrooms                        0.86\nlog_price                        1.14\nreview_scores_cleanliness        1.11\nreview_scores_location           0.91\nreview_scores_value              0.92\ninstant                          1.42\ndtype: float64\n\n\nAfter fitting the Poisson model we learn, in plain English, that switching on Instant Book is the single biggest lever: it lifts the expected review count by roughly 42 percent. A one-point bump in the cleanliness score nudges bookings up by about 11 percent, while each additional year on the platform adds a modest 1 to 2 percent of extra reviews. Bigger homes help at the margin—more bedrooms bring slightly more traffic—whereas adding bathrooms on top of the existing bedroom count appears to signal a pricier, slower-turnover property and nudges counts down. Price itself shows a small positive elasticity once value is controlled, and the classic room-type hierarchy (private &gt; entire place &gt; shared) persists but only at the ten-percent edge. In short, the Poisson coefficients translate into a story where convenience (Instant Book), visible quality (cleanliness), and sensible capacity win the day, while sheer luxury features do not automatically drive higher volume.\n\n\nLinear regression on log reviews\nWe now fit a linear regression model using the same set of predictors, to compare results and illustrate trade-offs. A direct linear model on the count of reviews would violate linearity and normality assumptions (since the outcome is non-negative and very skewed). Therefore, we use \\(\\log(\\text{number\\_of\\_reviews})\\) as the response. This means we are modeling the (natural) log of review count, which should yield coefficients that can be interpreted somewhat like elasticities (percent changes). Note that since we dropped zero-review listings, \\(\\log(\\text{reviews})\\) is defined (for 1 review, log = 0). Had we included zeros, we would need to add a small constant (e.g. log(review+1)) or use Tobit models, but we avoided that issue by excluding zeros earlier.\n\n\n================================================================================================\n                                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------------\nIntercept                        1.6229      0.126     12.857      0.000       1.375       1.870\nC(room_type)[T.Private room]    -0.0015      0.021     -0.071      0.944      -0.044       0.041\nC(room_type)[T.Shared room]     -0.0083      0.053     -0.157      0.875      -0.112       0.095\ndays                             0.0001   6.36e-06     18.050      0.000       0.000       0.000\nbedrooms                         0.0509      0.013      3.850      0.000       0.025       0.077\nbathrooms                       -0.1159      0.023     -5.069      0.000      -0.161      -0.071\nlog_price                        0.1509      0.019      8.158      0.000       0.115       0.187\nreview_scores_cleanliness        0.1364      0.009     15.003      0.000       0.119       0.154\nreview_scores_location          -0.1062      0.011     -9.629      0.000      -0.128      -0.085\nreview_scores_value             -0.0616      0.012     -5.141      0.000      -0.085      -0.038\ninstant                          0.3823      0.020     18.990      0.000       0.343       0.422\n================================================================================================\n\n\nThe linear model summary indicates an \\(R^2 = 0.036\\) (3.6%), meaning the predictors explain only a few percent of the variance in log-reviews. This is extremely low, highlighting that there is a lot of unexplained variability (no surprise given how many idiosyncratic factors affect a listing’s popularity). By contrast, the Poisson’s pseudo-\\(R^2\\) was much higher, but note that pseudo-\\(R^2\\) is not directly comparable to OLS \\(R^2\\) – they measure different things (deviance vs variance explained).\nQuick lift chart – turning coefficients into dollars\n\n\n\n\n\n\n\n\n\nAverage lift = 8.26 reviews over the period\n\n\nMost hosts could expect ~6–7 extra reviews (about +40 %) by flipping Instant Book on—substantial given the median listing only has 8.\n\n\nConclusion\nPutting everything together, hosts who activate Instant Book, keep their place immaculately clean, and offer a sensibly-sized listing at a price guests deem fair can expect materially more bookings—on the order of six to seven extra reviews (≈ 40 %) over the period analysed. Room-type differences are secondary, and charging a premium does not hurt as long as guests still feel the value is there. Because we removed zero-review rows, these insights apply to listings that have at least begun to attract guests; a full funnel analysis would model the “first-review” hurdle separately. Nonetheless, both the Poisson and log-linear models agree on the headline levers, giving us confidence that cleanliness and instant-booking convenience matter far more than whether the sofa faces north or the bath towels are monogrammed."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Juan Hernández Guizar",
    "section": "",
    "text": "Juan Hernández Guizar is an Aerospace Systems Engineer at Blue Origin, leading digital transformation and model-based systems engineering efforts on cislunar and beyond focused programs. Passionate about inclusive leadership and technical excellence, Juan also champions community-building."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Juan Hernández Guizar",
    "section": "Education",
    "text": "Education\nUC San Diego | San Diego, CA\nM.S. in Business Analytics (Expected Dec 2025)\nUC Merced | Merced, CA\nB.S. in Mechanical Engineering (May 2018)"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Juan Hernández Guizar",
    "section": "Experience",
    "text": "Experience\nBlue Origin | Aerospace Systems Engineer\nEl Segundo, CA | 2023 – Present\nLeading digital ecosystem initiatives and systems modeling for Project Starbridge.\nMercury Systems | Systems Engineering Manager\nTorrance, CA | 2021 – 2022\nManaged 10 engineers across multiple aerospace programs.\nRockwell Collins | Systems Engineer\nCedar Rapids, IA | 2018 – 2021\nScrum master and test lead for Embraer and Airbus platforms."
  }
]