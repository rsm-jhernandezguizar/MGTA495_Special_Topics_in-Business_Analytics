[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects for MGTA 459",
    "section": "",
    "text": "TBD\n\n\n \n\nAssignment 5\n\n\n\nJun 11, 2025\nJuan Hernández Guizar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTBD\n\n\n \n\nAssignment 4\n\n\n\nJun 4, 2025\nJuan Hernández Guizar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Nomial Logit (MNL) & Conjoint\n\n\n \n\nAssignment 3\n\n\n\nMay 28, 2025\nJuan Hernández Guizar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n \n\nAssignment 2\n\n\n\nMay 7, 2025\nJuan Hernández Guizar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n \n\nAssignment 1\n\n\n\nApr 23, 2025\nJuan Hernández Guizar\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/Assignment 2/Assignment_2.html",
    "href": "blog/Assignment 2/Assignment_2.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nTo start we will review the first 5 data points collected in a table format:\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nBelow is the distribution of patents among Blueprinty customers vs non-customers. The histogram shows that Blueprinty customers (light orange bars) tend to have more patents on average than non-customers (light blue bars).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn average, Blueprinty customers have approximately 4.13 patents over 5 years, compared to about 3.47 for non-customers. While this naive comparison suggests customers produce more patents, we must consider that Blueprinty’s customers may differ systematically in other ways (e.g. perhaps they are older firms or clustered in certain regions).\nLet’s examine the age and regional composition of customers vs non-customers.\n\n\n\n\n\n\n\n\n\nBlueprinty customers have a slightly higher mean age since incorporation (about 26.9 years) than non-customers (26.1 years), but the age distributions largely overlap (both groups are typically 20–30 years old, with only minor differences). This suggests that firm age might not differ dramatically by customer status, though we will account for age in the analysis.\nRegionally, there are stark differences in who adopts Blueprinty’s software.\n\n\n\n\n\n\n\n\n\nCounts of firms by region and Blueprinty customer status. In the Northeast region, the green bar (Blueprinty customers) is higher than the blue bar (non-customers), indicating a large share of Blueprinty’s users are in the Northeast. In contrast, in all other regions (Midwest, South, Southwest, Northwest) the majority of firms are non-customers. This reveals that Blueprinty’s customer base is heavily concentrated in the Northeast, which suggests potential selection bias by region.\nIndeed, about 68% of Blueprinty’s customers are located in the Northeast, whereas only ~27% of non-customer firms are in the Northeast. Other regions (Midwest, South, Southwest, Northwest) are under-represented among customers relative to non-customers. This imbalance means any raw difference in patent counts could partly reflect regional effects. In summary, Blueprinty customers tend to have slightly older firms (though age differences are minor) and are much more likely to be in the Northeast region. We will need to control for these factors when analyzing the effect of Blueprinty’s software on patent output.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nBelow, \\(Y_i\\) is the patent count for firm \\(i\\) and \\(\\lambda\\) is the average number of patents per firm in five years.\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(Y_i\\)\nObserved patent count for firm \\(i\\) (integer \\(\\ge 0\\))\n\n\n\\(n\\)\nTotal number of firms (length of \\(\\mathbf y\\))\n\n\n\\(\\lambda\\)\nPoisson rate parameter — the mean (and variance) of the distribution\n\n\n\\(\\mathbf y\\)\nColumn vector of all counts: \\(\\mathbf y = (\\,y_{1},\\,y_{2},\\,\\dots,\\,y_{n})^{\\!\\top}\\)\n\n\n\\(\\mathcal L(\\lambda;\\mathbf y)\\)\nLikelihood of the entire dataset, given \\(\\lambda\\)\n\n\n\\(\\ell(\\lambda)\\)\nLog-likelihood, \\(\\ell(\\lambda)=\\log \\mathcal L(\\lambda;\\mathbf y)\\)\n\n\n\n\\[\nP\\!\\bigl(Y_i=y_i \\mid \\lambda\\bigr)=\n  \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!},\n  \\qquad y_i=0,1,2,\\dots\n\\]\n\\[\n\\boxed{\n  \\mathcal L(\\lambda;\\mathbf y)=\n    e^{-n\\lambda}\\,\n    \\lambda^{\\sum_{i=1}^{n} y_i}\\!\n    \\Big/\n    \\prod_{i=1}^{n} y_i!\n}\n\\qquad\n\\boxed{\n  \\ell(\\lambda)=\n    \\sum_{i=1}^{n}\\!\n      \\bigl(\n        y_i\\log\\lambda-\\lambda-\\log y_i!\n      \\bigr)\n}\n\\]\nPutting all counts into one vector lets us write the likelihood compactly and pass the entire dataset to a single log-likelihood function. In the code below that function is called loglik_poisson. It follows the “parameter-vector” convention most optimisers expect: the one unknown, lambda, is stored as theta[0]. This style makes the function future-proof—if we later add more parameters we can just extend the theta vector without rewriting the optimiser call.\n\nimport numpy as np, math\n\n\ndef loglik_poisson(theta, y):\n    \"\"\"\n    Poisson log-likelihood\n\n    Parameters\n    ----------\n    theta : 1-element array-like\n        theta[0] = λ  (must be &gt; 0)\n    y     : 1-D numpy array of non-negative integers\n\n    Returns\n    -------\n    float\n        Scalar log-likelihood ℓ(λ)\n    \"\"\"\n    lam = float(theta[0])  # ← parallels 'mu &lt;- theta[1]'\n    if lam &lt;= 0:\n        return -np.inf  # guard just like s2&gt;0 in Normal case\n\n    n = y.size\n    # ll = Σ(y_i log λ) − n λ − Σ log(y_i!)\n    ll = np.sum(y * np.log(lam)) - n * lam - np.sum([math.lgamma(k + 1) for k in y])\n    return ll\n\nThe curve below shows how the log-likelihood changes as we slide λ across plausible values. It rises steeply, flattens, and then falls—peaking (unsurprisingly) right where λ equals the sample mean (~3.7 patents). That single highest point is the Maximum-Likelihood Estimate: the value of λ that makes the observed patent counts most probable under a Poisson model.\n\n\n\n\n\n\n\n\n\n\\[\nP\\!\\bigl(Y_i = y_i \\mid \\lambda\\bigr)=\n  \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!},\n  \\qquad y_i = 0,1,2,\\dots\n\\]\nDifferentiating the log-likelihood\n\\[\n\\ell(\\lambda)=\\sum_{i=1}^{n}\n  \\bigl(\n    y_i\\log\\lambda-\\lambda-\\log y_i!\n  \\bigr)\n\\]\nwith respect to () and setting the derivative to zero gives\n\\[\n\\frac{\\partial \\ell}{\\partial \\lambda}\n  \\;=\\;\n  \\frac{\\sum_{i=1}^{n}y_i}{\\lambda}-n\n  \\;=\\;0\n  \\;\\Longrightarrow\\;\n  \\boxed{\\hat\\lambda=\\bar y}\n\\]\nso the maximum-likelihood estimate is nothing more than the sample mean of the counts. The first code cell reflects that algebra exactly: y.mean() is computed and printed as the Analytic MLE, which for our data equals 3.6847 patents per firm.\n\n\nAnalytic MLE   λ̂ = 3.6847\n\n\n\n\nOptimiser MLE  λ̂ = 3.6847\n\n\nThe second cell tackles the same task numerically. scipy.optimize.minimize_scalar is instructed to minimise the negative log-likelihood (equivalently maximise ()), searching over the interval ([10^{-4},10]). Because the optimiser treats () as a scalar, we wrap it in a one-element list when passing it to loglik_poisson. After a quick line search it returns an Optimiser MLE of 3.6847, matching the analytic result to four decimal places—strong confirmation that the calculus and the numerical optimisation tell the same story.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\nA covariate (sometimes called a feature or explanatory variable) is simply an observed attribute we believe helps explain the outcome. Here our covariates are age, age ² (to capture curvature), a set of region dummies, and a binary flag for Blueprinty customer status. By stacking these in a matrix \\(X\\) and multiplying by a coefficient vector \\(\\boldsymbol\\beta\\), we let each firm have its own mean rate \\(\\lambda_i=\\exp(X_i^{!\\top}\\boldsymbol\\beta)\\)—the exponential ensures every \\(\\lambda_i\\) stays positive.\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(Y_i\\)\nObserved patent count for firm \\(i\\) (integer \\(\\ge 0\\))\n\n\n\\(X_i\\)\nRow vector of covariates for firm \\(i\\) (intercept, age, age\\(^{2}\\), region dummies, Blueprinty flag)\n\n\n\\(\\boldsymbol\\beta\\)\nColumn vector of coefficients (one per covariate)\n\n\n\\(\\lambda_i\\)\nMean patents for firm \\(i\\): \\(\\lambda_i = \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\\)\n\n\n\\(n\\)\nTotal number of firms (rows of \\(X\\))\n\n\n\\(\\mathbf y\\)\nColumn vector of all counts: \\(\\mathbf y = (\\,y_{1},\\,y_{2},\\,\\dots,\\,y_{n})^{\\!\\top}\\)\n\n\n\\(X\\)\nDesign matrix that stacks all \\(X_i\\) rows\n\n\n\\(\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\\)\nLikelihood of the entire dataset, given \\(\\boldsymbol\\beta\\)\n\n\n\\(\\ell(\\boldsymbol\\beta)\\)\nLog-likelihood, \\(\\ell(\\boldsymbol\\beta)=\\log \\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\\)\n\n\n\nExpanding from a constant‐rate model to Poisson regression swaps the single parameter \\(\\lambda\\) for a whole vector of coefficients \\(\\boldsymbol\\beta\\).\nEach firm now gets its own mean rate through the inverse-link function\n\\(\\lambda_i=\\exp(X_i^{\\!\\top}\\boldsymbol\\beta)\\), guaranteeing positivity while letting the linear predictor \\(X_i^{\\!\\top}\\boldsymbol\\beta\\) wander over the real line.\nThe covariate matrix \\(X\\) holds an intercept, age, age², a set of region dummies, and a Blueprinty-customer flag, so any of those characteristics can nudge the expected patent count up or down.\n\\[\nY_i \\,\\bigl|\\, X_i \\;\\sim\\; \\operatorname{Poisson}\\!\\bigl(\\lambda_i\\bigr),\n\\qquad\n\\lambda_i \\;=\\; \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr),\n\\qquad i = 1,\\dots,n.\n\\]\n\\[\n\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\n  \\;=\\;\n  \\prod_{i=1}^{n}\n    \\frac{e^{-\\lambda_i}\\,\\lambda_i^{\\,Y_i}}{Y_i!},\n\\qquad\n\\ell(\\boldsymbol\\beta)\n  \\;=\\;\n  \\sum_{i=1}^{n}\n    \\Bigl(\n      Y_i\\,X_i^{\\!\\top}\\boldsymbol\\beta\n      \\;-\\;\n      \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\n      \\;-\\;\n      \\log Y_i!\n    \\Bigr).\n\\]\nThe code block that follows translates this math into Python.\nloglik_poisson_reg(beta, y, X) now takes both the response vector and the covariate matrix, computes the linear predictor \\(X\\boldsymbol\\beta\\), exponentiates to obtain \\(\\boldsymbol\\lambda\\), and returns the scalar log-likelihood. Passing that function to an optimiser (e.g. scipy.optimize.minimize) yields the maximum-likelihood estimates for the full coefficient vector \\(\\boldsymbol\\beta\\).\n\nimport numpy as np, math\n\n\ndef loglik_poisson_reg(beta, y, X):\n    \"\"\"\n    Poisson regression log-likelihood.\n\n    beta : 1-D array, length p           (coefficients)\n    y    : 1-D array, length n           (counts)\n    X    : 2-D array, shape (n, p)       (covariate matrix)\n\n    Returns\n    -------\n    float : scalar log-likelihood ℓ(β)\n    \"\"\"\n    eta = X @ beta  # linear predictor  η = Xβ  (shape n)\n    lam = np.exp(eta)  # inverse link  λ = exp(η)\n    if np.any(lam &lt;= 0):\n        return -np.inf  # numerical safety\n    ll = np.sum(y * eta - lam - [math.lgamma(k + 1) for k in y])\n    return ll\n\nWith this function we can now hand the entire β vector to an optimiser (e.g. scipy.optimize.minimize) to obtain maximum-likelihood estimates, just as we did for the single-parameter case—only now the model flexes with age, geography, and Blueprinty adoption.\n\\[\n\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\n  = \\prod_{i=1}^{n}\n      \\frac{e^{-\\lambda_i}\\lambda_i^{\\,Y_i}}{Y_i!},\n\\qquad\n\\ell(\\boldsymbol\\beta)\n  = \\sum_{i=1}^{n}\n      \\Bigl(\n        Y_i\\,X_i^{\\!\\top}\\boldsymbol\\beta\n        - \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\n        - \\log Y_i!\n      \\Bigr).\n\\]\nA Hessian is the log-likelihood’s curvature map. Picture the likelihood surface as a hill; the Hessian tells us how sharply that hill drops away in every parameter direction. Formally\n\\[\nH(\\hat{\\boldsymbol\\beta})\n  = -\n    \\frac{\\partial^2\\ell(\\boldsymbol\\beta)}\n         {\\partial\\boldsymbol\\beta\\,\\partial\\boldsymbol\\beta^{\\!\\top}}\n  \\Biggr\\rvert_{\\;\\boldsymbol\\beta=\\hat{\\boldsymbol\\beta}},\n\\]\nand its negative inverse is the large-sample covariance of the MLEs, so ((_j)=).\n\n\n\n\n\n\n\n\n\nCoefficient\nStd.Error\n\n\n\n\nconst\n-0.5089\n0.1870\n\n\nage\n0.1486\n0.0140\n\n\nage2\n-0.2970\n0.0260\n\n\niscustomer\n0.2076\n0.0330\n\n\nregion_Northeast\n0.0292\n0.0465\n\n\nregion_Northwest\n-0.0176\n0.0572\n\n\nregion_South\n0.0566\n0.0558\n\n\nregion_Southwest\n0.0506\n0.0501\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nMon, 19 May 2025\nDeviance:\n2143.3\n\n\nTime:\n19:22:15\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nx1\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nx2\n-0.2970\n0.026\n-11.513\n0.000\n-0.348\n-0.246\n\n\nx3\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nx4\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nx5\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nx6\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nx7\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\nAfter rescaling age² and bounding the search, the Poisson regression converges cleanly. Key take-aways:\n\nAge (+) and Age² (−) form a concave pattern—patenting rises, peaks mid-20s, then tapers.\nRegion dummies shrink toward zero once we explicitly control for Blueprinty usage, implying geography itself isn’t the driver; the earlier Northeast spike simply reflected the high concentration of customers there.\nBlueprinty customer (+0.208) yields ≈ 23 % higher expected patents, highly significant even after all other controls.\n\nHand-rolled MLEs, Hessian-based standard errors, and statsmodels GLM all agree, giving us confidence in the estimates and the narrative.\nTo translate the log-coefficient into “extra patents,” we ran a simple counter-factual:\n\nX₀: keep every firm’s age and region but set iscustomer = 0.\n\nX₁: identical matrix but flip iscustomer = 1.\n\nPredict \\(\\hat y_0=\\exp(X_0\\hat\\beta)\\) and \\(\\hat y_1=\\exp(X_1\\hat\\beta)\\).\n\nTake the firm-by-firm difference \\(\\hat y_1-\\hat y_0\\) and average.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-customer\nBlueprinty\nLift\n\n\n\n\n0\n3.44\n4.23\n0.79\n\n\n\n\n\n\n\nThe result: +0.82 patents per firm over five years, about a 22 % lift relative to the baseline mean. The density plot below shows most firms gain between 0.5 and 1.1 extra patents, with a long but light right tail for the largest firms.\n\n\n\nAfter accounting for firm age and regional differences, using Blueprinty still delivers about one additional granted patent every five years. For most engineering shops that’s a solid, tangible boost—enough to nudge a “nice idea” into a fully protected asset on the balance sheet."
  },
  {
    "objectID": "blog/Assignment 2/Assignment_2.html#blueprinty-case-study",
    "href": "blog/Assignment 2/Assignment_2.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nTo start we will review the first 5 data points collected in a table format:\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nBelow is the distribution of patents among Blueprinty customers vs non-customers. The histogram shows that Blueprinty customers (light orange bars) tend to have more patents on average than non-customers (light blue bars).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn average, Blueprinty customers have approximately 4.13 patents over 5 years, compared to about 3.47 for non-customers. While this naive comparison suggests customers produce more patents, we must consider that Blueprinty’s customers may differ systematically in other ways (e.g. perhaps they are older firms or clustered in certain regions).\nLet’s examine the age and regional composition of customers vs non-customers.\n\n\n\n\n\n\n\n\n\nBlueprinty customers have a slightly higher mean age since incorporation (about 26.9 years) than non-customers (26.1 years), but the age distributions largely overlap (both groups are typically 20–30 years old, with only minor differences). This suggests that firm age might not differ dramatically by customer status, though we will account for age in the analysis.\nRegionally, there are stark differences in who adopts Blueprinty’s software.\n\n\n\n\n\n\n\n\n\nCounts of firms by region and Blueprinty customer status. In the Northeast region, the green bar (Blueprinty customers) is higher than the blue bar (non-customers), indicating a large share of Blueprinty’s users are in the Northeast. In contrast, in all other regions (Midwest, South, Southwest, Northwest) the majority of firms are non-customers. This reveals that Blueprinty’s customer base is heavily concentrated in the Northeast, which suggests potential selection bias by region.\nIndeed, about 68% of Blueprinty’s customers are located in the Northeast, whereas only ~27% of non-customer firms are in the Northeast. Other regions (Midwest, South, Southwest, Northwest) are under-represented among customers relative to non-customers. This imbalance means any raw difference in patent counts could partly reflect regional effects. In summary, Blueprinty customers tend to have slightly older firms (though age differences are minor) and are much more likely to be in the Northeast region. We will need to control for these factors when analyzing the effect of Blueprinty’s software on patent output.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nBelow, \\(Y_i\\) is the patent count for firm \\(i\\) and \\(\\lambda\\) is the average number of patents per firm in five years.\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(Y_i\\)\nObserved patent count for firm \\(i\\) (integer \\(\\ge 0\\))\n\n\n\\(n\\)\nTotal number of firms (length of \\(\\mathbf y\\))\n\n\n\\(\\lambda\\)\nPoisson rate parameter — the mean (and variance) of the distribution\n\n\n\\(\\mathbf y\\)\nColumn vector of all counts: \\(\\mathbf y = (\\,y_{1},\\,y_{2},\\,\\dots,\\,y_{n})^{\\!\\top}\\)\n\n\n\\(\\mathcal L(\\lambda;\\mathbf y)\\)\nLikelihood of the entire dataset, given \\(\\lambda\\)\n\n\n\\(\\ell(\\lambda)\\)\nLog-likelihood, \\(\\ell(\\lambda)=\\log \\mathcal L(\\lambda;\\mathbf y)\\)\n\n\n\n\\[\nP\\!\\bigl(Y_i=y_i \\mid \\lambda\\bigr)=\n  \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!},\n  \\qquad y_i=0,1,2,\\dots\n\\]\n\\[\n\\boxed{\n  \\mathcal L(\\lambda;\\mathbf y)=\n    e^{-n\\lambda}\\,\n    \\lambda^{\\sum_{i=1}^{n} y_i}\\!\n    \\Big/\n    \\prod_{i=1}^{n} y_i!\n}\n\\qquad\n\\boxed{\n  \\ell(\\lambda)=\n    \\sum_{i=1}^{n}\\!\n      \\bigl(\n        y_i\\log\\lambda-\\lambda-\\log y_i!\n      \\bigr)\n}\n\\]\nPutting all counts into one vector lets us write the likelihood compactly and pass the entire dataset to a single log-likelihood function. In the code below that function is called loglik_poisson. It follows the “parameter-vector” convention most optimisers expect: the one unknown, lambda, is stored as theta[0]. This style makes the function future-proof—if we later add more parameters we can just extend the theta vector without rewriting the optimiser call.\n\nimport numpy as np, math\n\n\ndef loglik_poisson(theta, y):\n    \"\"\"\n    Poisson log-likelihood\n\n    Parameters\n    ----------\n    theta : 1-element array-like\n        theta[0] = λ  (must be &gt; 0)\n    y     : 1-D numpy array of non-negative integers\n\n    Returns\n    -------\n    float\n        Scalar log-likelihood ℓ(λ)\n    \"\"\"\n    lam = float(theta[0])  # ← parallels 'mu &lt;- theta[1]'\n    if lam &lt;= 0:\n        return -np.inf  # guard just like s2&gt;0 in Normal case\n\n    n = y.size\n    # ll = Σ(y_i log λ) − n λ − Σ log(y_i!)\n    ll = np.sum(y * np.log(lam)) - n * lam - np.sum([math.lgamma(k + 1) for k in y])\n    return ll\n\nThe curve below shows how the log-likelihood changes as we slide λ across plausible values. It rises steeply, flattens, and then falls—peaking (unsurprisingly) right where λ equals the sample mean (~3.7 patents). That single highest point is the Maximum-Likelihood Estimate: the value of λ that makes the observed patent counts most probable under a Poisson model.\n\n\n\n\n\n\n\n\n\n\\[\nP\\!\\bigl(Y_i = y_i \\mid \\lambda\\bigr)=\n  \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!},\n  \\qquad y_i = 0,1,2,\\dots\n\\]\nDifferentiating the log-likelihood\n\\[\n\\ell(\\lambda)=\\sum_{i=1}^{n}\n  \\bigl(\n    y_i\\log\\lambda-\\lambda-\\log y_i!\n  \\bigr)\n\\]\nwith respect to () and setting the derivative to zero gives\n\\[\n\\frac{\\partial \\ell}{\\partial \\lambda}\n  \\;=\\;\n  \\frac{\\sum_{i=1}^{n}y_i}{\\lambda}-n\n  \\;=\\;0\n  \\;\\Longrightarrow\\;\n  \\boxed{\\hat\\lambda=\\bar y}\n\\]\nso the maximum-likelihood estimate is nothing more than the sample mean of the counts. The first code cell reflects that algebra exactly: y.mean() is computed and printed as the Analytic MLE, which for our data equals 3.6847 patents per firm.\n\n\nAnalytic MLE   λ̂ = 3.6847\n\n\n\n\nOptimiser MLE  λ̂ = 3.6847\n\n\nThe second cell tackles the same task numerically. scipy.optimize.minimize_scalar is instructed to minimise the negative log-likelihood (equivalently maximise ()), searching over the interval ([10^{-4},10]). Because the optimiser treats () as a scalar, we wrap it in a one-element list when passing it to loglik_poisson. After a quick line search it returns an Optimiser MLE of 3.6847, matching the analytic result to four decimal places—strong confirmation that the calculus and the numerical optimisation tell the same story.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\nA covariate (sometimes called a feature or explanatory variable) is simply an observed attribute we believe helps explain the outcome. Here our covariates are age, age ² (to capture curvature), a set of region dummies, and a binary flag for Blueprinty customer status. By stacking these in a matrix \\(X\\) and multiplying by a coefficient vector \\(\\boldsymbol\\beta\\), we let each firm have its own mean rate \\(\\lambda_i=\\exp(X_i^{!\\top}\\boldsymbol\\beta)\\)—the exponential ensures every \\(\\lambda_i\\) stays positive.\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(Y_i\\)\nObserved patent count for firm \\(i\\) (integer \\(\\ge 0\\))\n\n\n\\(X_i\\)\nRow vector of covariates for firm \\(i\\) (intercept, age, age\\(^{2}\\), region dummies, Blueprinty flag)\n\n\n\\(\\boldsymbol\\beta\\)\nColumn vector of coefficients (one per covariate)\n\n\n\\(\\lambda_i\\)\nMean patents for firm \\(i\\): \\(\\lambda_i = \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\\)\n\n\n\\(n\\)\nTotal number of firms (rows of \\(X\\))\n\n\n\\(\\mathbf y\\)\nColumn vector of all counts: \\(\\mathbf y = (\\,y_{1},\\,y_{2},\\,\\dots,\\,y_{n})^{\\!\\top}\\)\n\n\n\\(X\\)\nDesign matrix that stacks all \\(X_i\\) rows\n\n\n\\(\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\\)\nLikelihood of the entire dataset, given \\(\\boldsymbol\\beta\\)\n\n\n\\(\\ell(\\boldsymbol\\beta)\\)\nLog-likelihood, \\(\\ell(\\boldsymbol\\beta)=\\log \\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\\)\n\n\n\nExpanding from a constant‐rate model to Poisson regression swaps the single parameter \\(\\lambda\\) for a whole vector of coefficients \\(\\boldsymbol\\beta\\).\nEach firm now gets its own mean rate through the inverse-link function\n\\(\\lambda_i=\\exp(X_i^{\\!\\top}\\boldsymbol\\beta)\\), guaranteeing positivity while letting the linear predictor \\(X_i^{\\!\\top}\\boldsymbol\\beta\\) wander over the real line.\nThe covariate matrix \\(X\\) holds an intercept, age, age², a set of region dummies, and a Blueprinty-customer flag, so any of those characteristics can nudge the expected patent count up or down.\n\\[\nY_i \\,\\bigl|\\, X_i \\;\\sim\\; \\operatorname{Poisson}\\!\\bigl(\\lambda_i\\bigr),\n\\qquad\n\\lambda_i \\;=\\; \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr),\n\\qquad i = 1,\\dots,n.\n\\]\n\\[\n\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\n  \\;=\\;\n  \\prod_{i=1}^{n}\n    \\frac{e^{-\\lambda_i}\\,\\lambda_i^{\\,Y_i}}{Y_i!},\n\\qquad\n\\ell(\\boldsymbol\\beta)\n  \\;=\\;\n  \\sum_{i=1}^{n}\n    \\Bigl(\n      Y_i\\,X_i^{\\!\\top}\\boldsymbol\\beta\n      \\;-\\;\n      \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\n      \\;-\\;\n      \\log Y_i!\n    \\Bigr).\n\\]\nThe code block that follows translates this math into Python.\nloglik_poisson_reg(beta, y, X) now takes both the response vector and the covariate matrix, computes the linear predictor \\(X\\boldsymbol\\beta\\), exponentiates to obtain \\(\\boldsymbol\\lambda\\), and returns the scalar log-likelihood. Passing that function to an optimiser (e.g. scipy.optimize.minimize) yields the maximum-likelihood estimates for the full coefficient vector \\(\\boldsymbol\\beta\\).\n\nimport numpy as np, math\n\n\ndef loglik_poisson_reg(beta, y, X):\n    \"\"\"\n    Poisson regression log-likelihood.\n\n    beta : 1-D array, length p           (coefficients)\n    y    : 1-D array, length n           (counts)\n    X    : 2-D array, shape (n, p)       (covariate matrix)\n\n    Returns\n    -------\n    float : scalar log-likelihood ℓ(β)\n    \"\"\"\n    eta = X @ beta  # linear predictor  η = Xβ  (shape n)\n    lam = np.exp(eta)  # inverse link  λ = exp(η)\n    if np.any(lam &lt;= 0):\n        return -np.inf  # numerical safety\n    ll = np.sum(y * eta - lam - [math.lgamma(k + 1) for k in y])\n    return ll\n\nWith this function we can now hand the entire β vector to an optimiser (e.g. scipy.optimize.minimize) to obtain maximum-likelihood estimates, just as we did for the single-parameter case—only now the model flexes with age, geography, and Blueprinty adoption.\n\\[\n\\mathcal L(\\boldsymbol\\beta;\\mathbf y,X)\n  = \\prod_{i=1}^{n}\n      \\frac{e^{-\\lambda_i}\\lambda_i^{\\,Y_i}}{Y_i!},\n\\qquad\n\\ell(\\boldsymbol\\beta)\n  = \\sum_{i=1}^{n}\n      \\Bigl(\n        Y_i\\,X_i^{\\!\\top}\\boldsymbol\\beta\n        - \\exp\\!\\bigl(X_i^{\\!\\top}\\boldsymbol\\beta\\bigr)\n        - \\log Y_i!\n      \\Bigr).\n\\]\nA Hessian is the log-likelihood’s curvature map. Picture the likelihood surface as a hill; the Hessian tells us how sharply that hill drops away in every parameter direction. Formally\n\\[\nH(\\hat{\\boldsymbol\\beta})\n  = -\n    \\frac{\\partial^2\\ell(\\boldsymbol\\beta)}\n         {\\partial\\boldsymbol\\beta\\,\\partial\\boldsymbol\\beta^{\\!\\top}}\n  \\Biggr\\rvert_{\\;\\boldsymbol\\beta=\\hat{\\boldsymbol\\beta}},\n\\]\nand its negative inverse is the large-sample covariance of the MLEs, so ((_j)=).\n\n\n\n\n\n\n\n\n\nCoefficient\nStd.Error\n\n\n\n\nconst\n-0.5089\n0.1870\n\n\nage\n0.1486\n0.0140\n\n\nage2\n-0.2970\n0.0260\n\n\niscustomer\n0.2076\n0.0330\n\n\nregion_Northeast\n0.0292\n0.0465\n\n\nregion_Northwest\n-0.0176\n0.0572\n\n\nregion_South\n0.0566\n0.0558\n\n\nregion_Southwest\n0.0506\n0.0501\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nMon, 19 May 2025\nDeviance:\n2143.3\n\n\nTime:\n19:22:15\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nx1\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nx2\n-0.2970\n0.026\n-11.513\n0.000\n-0.348\n-0.246\n\n\nx3\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nx4\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nx5\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nx6\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nx7\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\nAfter rescaling age² and bounding the search, the Poisson regression converges cleanly. Key take-aways:\n\nAge (+) and Age² (−) form a concave pattern—patenting rises, peaks mid-20s, then tapers.\nRegion dummies shrink toward zero once we explicitly control for Blueprinty usage, implying geography itself isn’t the driver; the earlier Northeast spike simply reflected the high concentration of customers there.\nBlueprinty customer (+0.208) yields ≈ 23 % higher expected patents, highly significant even after all other controls.\n\nHand-rolled MLEs, Hessian-based standard errors, and statsmodels GLM all agree, giving us confidence in the estimates and the narrative.\nTo translate the log-coefficient into “extra patents,” we ran a simple counter-factual:\n\nX₀: keep every firm’s age and region but set iscustomer = 0.\n\nX₁: identical matrix but flip iscustomer = 1.\n\nPredict \\(\\hat y_0=\\exp(X_0\\hat\\beta)\\) and \\(\\hat y_1=\\exp(X_1\\hat\\beta)\\).\n\nTake the firm-by-firm difference \\(\\hat y_1-\\hat y_0\\) and average.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-customer\nBlueprinty\nLift\n\n\n\n\n0\n3.44\n4.23\n0.79\n\n\n\n\n\n\n\nThe result: +0.82 patents per firm over five years, about a 22 % lift relative to the baseline mean. The density plot below shows most firms gain between 0.5 and 1.1 extra patents, with a long but light right tail for the largest firms.\n\n\n\nAfter accounting for firm age and regional differences, using Blueprinty still delivers about one additional granted patent every five years. For most engineering shops that’s a solid, tangible boost—enough to nudge a “nice idea” into a fully protected asset on the balance sheet."
  },
  {
    "objectID": "blog/Assignment 2/Assignment_2.html#airbnb-case-study",
    "href": "blog/Assignment 2/Assignment_2.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nWe treat number of reviews as a stand-in for bookings and begin by exploring the 40,628-listing Airbnb-NYC dataset (features include listing age days, room type, bedrooms, bathrooms, nightly price, review scores for cleanliness / location / value, and an instant-bookable flag).\n\nHandling missing values – 76 listings lack bedrooms, 160 lack bathrooms, and about 10,200 lack all three review-score variables.\nMost of those 10 k are listings with zero reviews (9,481 rows, ≈ 23 % of the data).\nWe drop any row with a missing predictor to keep modeling simple, which chiefly removes those zero-review listings and leaves 30,160 listings (all with ≥ 1 review).\nShould we keep the zero-review rows?\nIncluding them would preserve information on brand-new hosts but requires imputing their absent review scores or using a two-part model.\nFor this tutorial we exclude them, accepting a bit of bias in exchange for cleaner predictors; we flag that trade-off for future work.\nFeature transformations –\ninstant_bookable is recoded from 't'/'f' to 0 / 1.\nNightly price is extremely right-skewed, so we model log_price instead, which stabilises variance and gives a roughly bell-shaped histogram.\ndays remains in raw units (median ≈ 3 years; one outlier appears at 117 years!), and no further transforms are applied at this stage.\n\nNext, let’s inspect the distribution of our key variables:\n\n\n\n\n\n\n\n\n\nLeft plot: Distribution of the number of reviews per listing (for listings with ≥1 review). The histogram is extremely right-skewed. A large fraction of listings have only a handful of reviews – for example, the median is 8 reviews, and 75% have ≤26 reviews. A long tail of popular listings have many more reviews (the maximum in this subset is 421). This heavy-tailed count distribution suggests that modeling approaches for count data (like Poisson regression) or a log-transformation may be appropriate. Note: ~23% of listings had 0 reviews (not shown here, as they were dropped for modeling), indicating many very new or less-booked listings.\nRight plot: Distribution of nightly price, in USD (left), and distribution of log-transformed price (right) for NYC Airbnb listings. The raw price distribution is highly skewed with most listings in the $50–$200 range and a few extreme outliers (up to $10,000). We limited the x-axis to $500 in the left plot for clarity, but even within this range the mass is concentrated at lower prices. The log-scale (natural log) of price, shown on the right, is much more symmetric and bell-shaped. This confirms that a log transformation of price will likely make modeling easier: a unit change in log_price corresponds to a multiplicative change in actual price, and we expect a more linear relationship with outcome variables on that scale.\nWith the data cleaned and initial insights gathered, we proceed to model the number of reviews (as a proxy for bookings) using two approaches: Poisson regression for count data, and linear regression. The response variable will be the count of reviews. In the linear model, we will use a log transformation of reviews to account for skewness, whereas the Poisson model will use the count directly with a log link function.\n\n\nPoisson Model\nA log link makes each coefficient a multiplicative bump.\n\\[\n\\ell(\\beta)=\\sum_i\\Bigl(y_iX_i’\\beta-\\exp(X_i’\\beta)-\\ln(y_i!)\\Bigr)\n\\]\n\n\n================================================================================================\n                                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------------------\nIntercept                        3.0166      0.019    156.966      0.000       2.979       3.054\nC(room_type)[T.Private room]     0.0874      0.003     25.854      0.000       0.081       0.094\nC(room_type)[T.Shared room]     -0.1033      0.009    -11.345      0.000      -0.121      -0.085\ndays                          5.056e-05   3.93e-07    128.807      0.000    4.98e-05    5.13e-05\nbedrooms                         0.0464      0.002     22.733      0.000       0.042       0.050\nbathrooms                       -0.1453      0.004    -38.805      0.000      -0.153      -0.138\nlog_price                        0.1309      0.003     45.347      0.000       0.125       0.137\nreview_scores_cleanliness        0.1088      0.001     72.531      0.000       0.106       0.112\nreview_scores_location          -0.0975      0.002    -58.982      0.000      -0.101      -0.094\nreview_scores_value             -0.0794      0.002    -43.513      0.000      -0.083      -0.076\ninstant                          0.3521      0.003    121.730      0.000       0.346       0.358\n================================================================================================\n\nIncidence-rate ratios (IRR)\n Intercept                       20.42\nC(room_type)[T.Private room]     1.09\nC(room_type)[T.Shared room]      0.90\ndays                             1.00\nbedrooms                         1.05\nbathrooms                        0.86\nlog_price                        1.14\nreview_scores_cleanliness        1.11\nreview_scores_location           0.91\nreview_scores_value              0.92\ninstant                          1.42\ndtype: float64\n\n\nAfter fitting the Poisson model we learn, in plain English, that switching on Instant Book is the single biggest lever: it lifts the expected review count by roughly 42 percent. A one-point bump in the cleanliness score nudges bookings up by about 11 percent, while each additional year on the platform adds a modest 1 to 2 percent of extra reviews. Bigger homes help at the margin—more bedrooms bring slightly more traffic—whereas adding bathrooms on top of the existing bedroom count appears to signal a pricier, slower-turnover property and nudges counts down. Price itself shows a small positive elasticity once value is controlled, and the classic room-type hierarchy (private &gt; entire place &gt; shared) persists but only at the ten-percent edge. In short, the Poisson coefficients translate into a story where convenience (Instant Book), visible quality (cleanliness), and sensible capacity win the day, while sheer luxury features do not automatically drive higher volume.\n\n\nLinear regression on log reviews\nWe now fit a linear regression model using the same set of predictors, to compare results and illustrate trade-offs. A direct linear model on the count of reviews would violate linearity and normality assumptions (since the outcome is non-negative and very skewed). Therefore, we use \\(\\log(\\text{number\\_of\\_reviews})\\) as the response. This means we are modeling the (natural) log of review count, which should yield coefficients that can be interpreted somewhat like elasticities (percent changes). Note that since we dropped zero-review listings, \\(\\log(\\text{reviews})\\) is defined (for 1 review, log = 0). Had we included zeros, we would need to add a small constant (e.g. log(review+1)) or use Tobit models, but we avoided that issue by excluding zeros earlier.\n\n\n================================================================================================\n                                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------------\nIntercept                        1.6229      0.126     12.857      0.000       1.375       1.870\nC(room_type)[T.Private room]    -0.0015      0.021     -0.071      0.944      -0.044       0.041\nC(room_type)[T.Shared room]     -0.0083      0.053     -0.157      0.875      -0.112       0.095\ndays                             0.0001   6.36e-06     18.050      0.000       0.000       0.000\nbedrooms                         0.0509      0.013      3.850      0.000       0.025       0.077\nbathrooms                       -0.1159      0.023     -5.069      0.000      -0.161      -0.071\nlog_price                        0.1509      0.019      8.158      0.000       0.115       0.187\nreview_scores_cleanliness        0.1364      0.009     15.003      0.000       0.119       0.154\nreview_scores_location          -0.1062      0.011     -9.629      0.000      -0.128      -0.085\nreview_scores_value             -0.0616      0.012     -5.141      0.000      -0.085      -0.038\ninstant                          0.3823      0.020     18.990      0.000       0.343       0.422\n================================================================================================\n\n\nThe linear model summary indicates an \\(R^2 = 0.036\\) (3.6%), meaning the predictors explain only a few percent of the variance in log-reviews. This is extremely low, highlighting that there is a lot of unexplained variability (no surprise given how many idiosyncratic factors affect a listing’s popularity). By contrast, the Poisson’s pseudo-\\(R^2\\) was much higher, but note that pseudo-\\(R^2\\) is not directly comparable to OLS \\(R^2\\) – they measure different things (deviance vs variance explained).\nQuick lift chart – turning coefficients into dollars\n\n\n\n\n\n\n\n\n\nAverage lift = 8.26 reviews over the period\n\n\nMost hosts could expect ~6–7 extra reviews (about +40 %) by flipping Instant Book on—substantial given the median listing only has 8.\n\n\nConclusion\nPutting everything together, hosts who activate Instant Book, keep their place immaculately clean, and offer a sensibly-sized listing at a price guests deem fair can expect materially more bookings—on the order of six to seven extra reviews (≈ 40 %) over the period analysed. Room-type differences are secondary, and charging a premium does not hurt as long as guests still feel the value is there. Because we removed zero-review rows, these insights apply to listings that have at least begun to attract guests; a full funnel analysis would model the “first-review” hurdle separately. Nonetheless, both the Poisson and log-linear models agree on the headline levers, giving us confidence that cleanliness and instant-booking convenience matter far more than whether the sofa faces north or the bath towels are monogrammed."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Juan Hernández Guizar",
    "section": "",
    "text": "Juan Hernández Guizar is an Aerospace Systems Engineer at Blue Origin, leading digital transformation and model-based systems engineering efforts on cislunar and beyond focused programs. Passionate about inclusive leadership and technical excellence, Juan also champions community-building."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Juan Hernández Guizar",
    "section": "Education",
    "text": "Education\nUC San Diego | San Diego, CA\nM.S. in Business Analytics (Expected Dec 2025)\nUC Merced | Merced, CA\nB.S. in Mechanical Engineering (May 2018)"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Juan Hernández Guizar",
    "section": "Experience",
    "text": "Experience\nBlue Origin | Aerospace Systems Engineer\nEl Segundo, CA | 2023 – Present\nLeading digital ecosystem initiatives and systems modeling for Project Starbridge.\nMercury Systems | Systems Engineering Manager\nTorrance, CA | 2021 – 2022\nManaged 10 engineers across multiple aerospace programs.\nRockwell Collins | Systems Engineer\nCedar Rapids, IA | 2018 – 2021\nScrum master and test lead for Embraer and Airbus platforms."
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Juan’s Resume",
    "section": "",
    "text": "Last updated: 2025-04-05\nDownload PDF file."
  },
  {
    "objectID": "Other/Assignment_1_Stuff/File_Conversion.html",
    "href": "Other/Assignment_1_Stuff/File_Conversion.html",
    "title": "Juan Hernández Guizar",
    "section": "",
    "text": "import pandas as pd\n\nkarlan_list_2007_csv = pd.read_stata(\"karlan_list_2007.dta\")\n\nkarlan_list_2007_csv.to_csv(\"karlan_list_2007_csv.csv\", index=False)"
  },
  {
    "objectID": "blog/Assignment 1/Assignment_1.html",
    "href": "blog/Assignment 1/Assignment_1.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn their study, Karlan and List discovered that announcing a matching grant significantly boosted both the probability and size of contributions, confirming that even simple price incentives can nudge donors into action. Intriguingly, however, larger match ratios (such as a $3:$1 match) did not outperform smaller ones (like $1:$1), suggesting that bigger “discounts” on giving may not always translate to bigger impacts. They also found the local political environment influenced donor responsiveness: individuals in conservative “red” states were more swayed by the matching offer than those in liberal “blue” states. This highlights that factors like community norms and trust can be just as critical as the financial structure of a fundraising campaign.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Assignment 1/Assignment_1.html#introduction",
    "href": "blog/Assignment 1/Assignment_1.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn their study, Karlan and List discovered that announcing a matching grant significantly boosted both the probability and size of contributions, confirming that even simple price incentives can nudge donors into action. Intriguingly, however, larger match ratios (such as a $3:$1 match) did not outperform smaller ones (like $1:$1), suggesting that bigger “discounts” on giving may not always translate to bigger impacts. They also found the local political environment influenced donor responsiveness: individuals in conservative “red” states were more swayed by the matching offer than those in liberal “blue” states. This highlights that factors like community norms and trust can be just as critical as the financial structure of a fundraising campaign.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Assignment 1/Assignment_1.html#data",
    "href": "blog/Assignment 1/Assignment_1.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nTo replicate Karlan and List’s findings, I first loaded their dataset and generated preliminary plots to get a feel for its contents. After displaying the first few rows to confirm the structure, I computed donation rates by treatment group and then prepared side-by-side visuals—bar plots for participation and histograms of donation amounts—to highlight the core outcome measures. These initial checks ensure that the data aligns with the original study’s composition before we proceed with more in-depth statistical testing and analysis.\nDetailed explanation of all the variables captured\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\nPreview of results from study\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\nBar plot – Proportion Who Donated by Treatment Group\n\n\n\n\n\n\n\n\n\nIn this bar plot, each bar shows the proportion of people in the treatment or control group who made a donation, illustrating the immediate difference in participation rates.\nHistogram – Donation Amounts (Among Donors Only)\n\n\n\n\n\n\n\n\n\nHere, the histogram reveals the distribution of how much donors gave, helping us detect outliers, skewness, or other patterns in giving behavior.\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nAs part of the balance test, I conducted a hand-computed two-sample t-test to determine whether the treatment and control groups differed significantly on the variable mrm2 (months since last donation). The results showed that the treatment group had a mean of 13.012 months, while the control group had a mean of 12.998 months. The calculated t-statistic was 0.120 with an associated p-value of 0.9049. Since the p-value is well above the 0.05 threshold, we fail to reject the null hypothesis and conclude that there is no statistically significant difference between the groups. This result supports the validity of the randomization mechanism, as it suggests that both groups were balanced on this pre-treatment variable.\n\n\nHand-Composed Two-Sample t-Test for mrm2\n=========================================\nTreatment Mean (mrm2): 13.012, n=33395\nControl Mean (mrm2):   12.998, n=16687\nt-statistic:           0.120\nDegrees of freedom:    33394.48\np-value (two-sided):   0.9049\n\n\nTo validate these results using a different method, I also ran a simple linear regression where mrm2 was regressed on the treatment variable. This approach tests the same hypothesis as the t-test—that the average months since last donation is equal across groups. The estimated treatment effect (0.014) is nearly identical to the difference in group means, and the associated p-value (0.905) confirms the same conclusion: there is no statistically significant difference between the groups. This reinforces the finding that the randomization successfully created balanced groups.\n\n\nLinear regression (OLS)\nData                 : karlan_list_2007_pretty\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082\n\n\nAs part of the balance test, I conducted a hand-computed two-sample t-test to determine whether the treatment and control groups differed significantly on the variable freq (number of prior donations). This represents the second variable tested for robustness, following the initial test on mrm2 (months since last donation). The results showed that the treatment group had a mean of 8.035 donations, while the control group had a mean of 8.047 donations. The calculated t-statistic was -0.111 with an associated p-value of 0.9117. Since the p-value is well above the 0.05 threshold, we fail to reject the null hypothesis and conclude that there is no statistically significant difference between the groups. The fact that both mrm2 and freq are balanced across treatment and control groups further validates the randomization mechanism, confirming that the two groups are comparable on key pre-treatment characteristics.\n\n\nHand-Composed Two-Sample t-Test for freq\n=========================================\nTreatment Mean (freq): 8.035, n=33396\nControl Mean (freq):   8.047, n=16687\nt-statistic:           -0.111\nDegrees of freedom:    33326.35\np-value (two-sided):   0.9117\n\n\nTo validate the results from the t-test, I also performed a linear regression of freq on the treatment variable. Like the t-test, this regression assesses whether there is a statistically significant difference in the number of prior donations between treatment and control groups. The coefficient on treatment (-0.012) closely matches the difference in group means, and the p-value (0.912) confirms the same conclusion: no significant difference exists. This consistency between the regression and the hand-calculated t-test reinforces the finding that the treatment assignment did not systematically influence pre-treatment donation frequency.\n\n\nLinear regression (OLS)\nData                 : karlan_list_2007_pretty\nResponse variable    : freq\nExplanatory variables: treatment\nNull hyp.: the effect of x on freq is zero\nAlt. hyp.: the effect of x on freq is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        8.047      0.088   91.231  &lt; .001  ***\ntreatment       -0.012      0.108   -0.111   0.912     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.012 df(1, 50081), p.value 0.912\nNr obs: 50,083\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese results conclude the Balance Test section and provide strong support for the success of the randomization mechanism."
  },
  {
    "objectID": "blog/Assignment 1/Assignment_1.html#experimental-results",
    "href": "blog/Assignment 1/Assignment_1.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nHere’s a short paragraph you can use to replace the todo sentence in the screenshot:\nThe bar plot below shows the proportion of individuals who made a donation in the treatment and control groups. This visualization offers an early look at the potential impact of being assigned to the treatment group, with a slightly higher donation rate observed. While this plot conveys similar information to the one presented in the Data section, it is revisited here in the context of hypothesis testing to begin assessing whether the observed difference is statistically significant.\n\n\n\n\n\n\n\n\n\nBased on the code used in our earlier balance checks, I ran a t‐test comparing the proportion who donated (gave == 1) across the treatment and control groups. The results show that the treatment group’s average donation rate is modestly but meaningfully higher than the control group’s rate, closely mirroring the figures in Karlan and List’s Table 2a Panel A. The p‐value from this test is well below the usual 5 percent threshold, indicating that the difference is unlikely to be by chance. In real terms, such a small bump in donation rates can significantly boost total contributions, demonstrating that even a simple intervention like a matching grant can alter donor behavior. This supports the broader conclusion that small “price” or matching signals can nudge people to act, thereby increasing charitable giving.\nHere’s a clear paragraph you can use to summarize the probit regression results in context with Table 3, Column 1 from the Karlan and List (2007) paper:\nI ran a probit regression to estimate whether being assigned to the treatment group significantly increased the likelihood of making a charitable donation. The model used gave as the binary outcome and treatment as the explanatory variable. The results show a positive and statistically significant coefficient of 0.0868 (p = 0.002), indicating that assignment to the treatment group is associated with a higher probability of donating. However, this estimate does not match the coefficient reported in Table 3, Column 1 of the original paper, which shows a much smaller effect size of 0.004 with a standard error of 0.001. Despite the model setup appearing consistent, the difference in results suggests there may be differences in the underlying implementation or sample filtering. Still, the significance of the treatment variable aligns with the paper’s broader conclusion: matching grants, even modest ones, can meaningfully shift donation behavior.\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nMon, 19 May 2025\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n19:22:07\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nTo evaluate the impact of different match ratios on donation behavior, I conducted both t-tests and a linear regression using dummy variables for the 1:1 (ratio1), 2:1 (ratio2), and 3:1 (ratio3) match groups. The t-tests showed that while donation rates were slightly higher in the 2:1 and 3:1 groups compared to 1:1, none of these differences were statistically significant. Donation probabilities ranged narrowly between 2.07% and 2.27%, suggesting that although the presence of a match increased giving, raising the match ratio did not lead to meaningful increases in participation.\nThe regression analysis confirmed these findings. Relative to the omitted baseline group, all three match treatments were associated with higher donation probabilities: +0.3 percentage points for 1:1 (p = 0.097), and +0.5 percentage points for both 2:1 and 3:1 (p-values = 0.006 and 0.005, respectively). However, the similarity in effect sizes between the 2:1 and 3:1 groups reinforces the idea of diminishing returns from more generous matches. Together, these results support the conclusion of Karlan and List (2007): while matching grants are effective overall, increasing the match ratio beyond 1:1 does not significantly boost donor participation.\n\n\n\n=== Checking ratio 2:1 vs. ratio 1:1 ===\nObservations in 2:1: 11134\nObservations in 1:1: 11133\nMean(gave) for 2:1: 0.0226\nMean(gave) for 1:1: 0.0207\nT-statistic: 0.9650  |  P-value: 0.3345\n\n=== Checking ratio 3:1 vs. ratio 1:1 ===\nObservations in 3:1: 11129\nObservations in 1:1: 11133\nMean(gave) for 3:1: 0.0227\nMean(gave) for 1:1: 0.0207\nT-statistic: 1.0150  |  P-value: 0.3101\n\n=== Checking ratio 2:1 vs. ratio 3:1 ===\nObservations in 2:1: 11134\nObservations in 3:1: 11129\nMean(gave) for 2:1: 0.0226\nMean(gave) for 3:1: 0.0227\nT-statistic: -0.0501  |  P-value: 0.96\n\n\nTo assess the effect of different match ratios on donation likelihood, I ran an OLS regression of the binary outcome gave on dummy variables ratio1, ratio2, and ratio3, representing 1:1, 2:1, and 3:1 match offers, respectively. The regression results show that all three match ratios are associated with higher donation rates compared to the omitted group, with estimated effects of 0.003 (p = 0.097) for 1:1, 0.005 (p = 0.006) for 2:1, and 0.005 (p = 0.005) for 3:1. The 2:1 and 3:1 coefficients are statistically significant at the 1% level, while 1:1 is only marginally significant at the 10% level. Despite these coefficients being small in magnitude (increasing likelihood of donation by 0.3 to 0.5 percentage points), they are meaningful in the context of mass fundraising. However, the similarity in effect size between 2:1 and 3:1 suggests diminishing returns to more generous matching—larger match ratios do not yield proportionally greater increases in donation rates. This conclusion is reinforced by direct calculation: the response rate difference between 1:1 and 2:1 is approximately 0.002, and between 2:1 and 3:1 is nearly zero (–0.0001), which mirrors the difference in regression coefficients. These findings align with the original study’s conclusion that while the presence of a match increases donations, increasing the match ratio beyond 1:1 offers no additional boost in donor participation.\n\n\nLinear regression (OLS)\nData                 : karlan_list_2007_pretty\nResponse variable    : gave\nExplanatory variables: ratio1, ratio2, ratio3\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\nratio1           0.003      0.002    1.661   0.097    .\nratio2           0.005      0.002    2.744   0.006   **\nratio3           0.005      0.002    2.802   0.005   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.665 df(3, 50079), p.value 0.012\nNr obs: 50,083\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo assess the effectiveness of different matched donation sizes, I calculated the response rate differences directly from the data and compared those with differences from the regression coefficients. Direct computation from the data shows that moving from a 1:1 to a 2:1 match ratio increases the donation response rate by approximately 0.0019 (0.19 percentage points), while moving from 2:1 to 3:1 provides virtually no additional benefit (around 0.0001, or 0.01 percentage points).\nThe coefficients indicate that shifting from a 1:1 to a 2:1 match leads to an increase in donation likelihood by about 0.0019 (0.19 percentage points), which is statistically significant (p = 0.006). However, increasing the match ratio further, from 2:1 to 3:1, offers almost no additional improvement (just 0.0001), indicating diminishing returns for more generous match ratios. These results align closely with Karlan and List’s original conclusions, demonstrating that while the presence of a match is effective at boosting donation rates, making the match more generous beyond a certain point does not significantly increase donor participation.\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.665\nDate:                Mon, 19 May 2025   Prob (F-statistic):             0.0118\nTime:                        19:22:08   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50079   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\nratio1         0.0029      0.002      1.661      0.097      -0.001       0.006\nratio2         0.0048      0.002      2.744      0.006       0.001       0.008\nratio3         0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\nOmnibus:                    59812.754   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316693.217\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.438   Cond. No.                         4.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n=== Response Rate Differences (From Regression Coefficients) ===\nDifference (2:1 vs. 1:1): 0.0019\nDifference (3:1 vs. 2:1): 0.0001\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nFrom the results of the t-test comparing donation amounts between the treatment and control groups, we observe a mean donation of approximately $0.97 for the treatment group and $0.81 for the control group, indicating a moderate increase in donation amounts due to treatment. The calculated t-statistic is 1.9183, and the associated p-value is 0.05509, just above the conventional significance threshold of 0.05. This result suggests that while there is some evidence that the treatment leads to higher average donations, the difference is only marginally statistically significant. Thus, we learn that matching grants have the potential not only to increase participation but possibly also to increase donation amounts among all individuals (though evidence for this latter effect is weaker). Further investigation, potentially with larger sample sizes or more targeted treatments, would be beneficial to clarify this effect.\n\n\nT-test comparing donation amounts between treatment and control groups\nT-statistic: 1.9183\np-value: 0.05509\nMean donation (Treatment): 0.9669\nMean donation (Control): 0.8133\n\n\nThe regression results presented indicate that, among individuals who chose to donate (conditional donors), the estimated average donation for those in the control group is approximately $45.54 (intercept). The coefficient for the treatment variable is -1.668, suggesting that, on average, donors assigned to the treatment group donate about $1.67 less than those in the control group. However, this difference is not statistically significant, as indicated by the large p-value (0.561). Therefore, we cannot confidently conclude that the treatment had any substantial effect on the donation amount among people who donated.\nImportantly, the treatment coefficient here does have a causal interpretation because the original experimental design involves random assignment of individuals to treatment and control groups. Thus, the observed effect (or lack thereof) can reasonably be interpreted as causal. In this case, we learn that, conditional on donating, the matching treatment does not significantly influence the donation size—supporting the earlier observation that the main benefit of matching grants appears primarily in encouraging participation, rather than increasing amounts among existing donors.\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Mon, 19 May 2025   Prob (F-statistic):              0.561\nTime:                        19:22:08   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\ntodo: Make two plots: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "blog/Assignment 1/Assignment_1.html#simulation-experiment-law-of-large-numbers-central-limit-theorem",
    "href": "blog/Assignment 1/Assignment_1.html#simulation-experiment-law-of-large-numbers-central-limit-theorem",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment (Law of Large Numbers + Central Limit Theorem)",
    "text": "Simulation Experiment (Law of Large Numbers + Central Limit Theorem)\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem. Suppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p = 0.018 that a donation is made, and respondents who do get a charitable donation match of any size have a probability p = 0.022 of donating.\nThe first plot illustrates the Law of Large Numbers by showing the cumulative average difference between simulated treatment and control groups across 10,000 draws. Initially, the differences fluctuate widely due to small sample sizes, but as the number of observations grows, the cumulative average difference gradually stabilizes and converges near the true difference of 0.004.\nThe next four histograms demonstrate the Central Limit Theorem. Each histogram displays the distribution of 1000 repeated samples of average differences between the two groups for varying sample sizes (50, 200, 500, and 1000). For smaller samples (n=50), the distribution is wide, irregular, and centered near the true difference (0.004) but with considerable variation. As sample size increases (n=200, 500, and 1000), the distributions become progressively narrower and more symmetrical, clearly approximating a normal distribution and tightly centering around the true mean difference of 0.004. These results illustrate how increasing sample size improves precision, reduces variability, and provides the basis for robust statistical inference, as embodied by the t-statistic."
  },
  {
    "objectID": "blog/Assignment 1/Assignment_1.html#summary-conclusion",
    "href": "blog/Assignment 1/Assignment_1.html#summary-conclusion",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Summary / Conclusion",
    "text": "Summary / Conclusion\nIn summary, this replication of Karlan and List’s (2007) study successfully validated their key findings. Matching grants indeed significantly increased the likelihood of donors contributing, confirming the power of simple financial incentives. Interestingly, while the presence of a match effectively boosted participation, increasing the match ratio beyond a basic 1:1 offer showed minimal additional impact, highlighting diminishing returns for more generous incentives. Further, the analysis indicated that the treatment primarily influences the decision to donate rather than the amount donated among those who chose to give. Lastly, simulation experiments effectively illustrated fundamental statistical principles: the Law of Large Numbers demonstrated how averages stabilize around true effects with larger samples, while the Central Limit Theorem showed how sampling distributions become increasingly normal and precise as sample sizes grow. Together, these analyses reinforce the importance of robust experimental design and statistical reasoning in evaluating charitable fundraising strategies."
  },
  {
    "objectID": "blog/Assignment 3/Assignment_3.html",
    "href": "blog/Assignment 3/Assignment_3.html",
    "title": "Multi-Nomial Logit (MNL) & Conjoint",
    "section": "",
    "text": "This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/Assignment 3/Assignment_3.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/Assignment 3/Assignment_3.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multi-Nomial Logit (MNL) & Conjoint",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/Assignment 3/Assignment_3.html#simulate-conjoint-data",
    "href": "blog/Assignment 3/Assignment_3.html#simulate-conjoint-data",
    "title": "Multi-Nomial Logit (MNL) & Conjoint",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\n\n# Standard scientific stack\nimport numpy as np\nimport pandas as pd\nfrom itertools import product\n\n# Reproducibility\nnp.random.seed(123)  # same seed as the original R script\n\n#| label: attrs\n\n# Streaming brand (Netflix=N, Prime=P, Hulu=H)\nbrand_levels = [\"N\", \"P\", \"H\"]\n\n# Advertisement presence\nad_levels = [\"Yes\", \"No\"]\n\n# Monthly price in USD (8, 12, …, 32)\nprice_levels = np.arange(8, 33, 4)\n\n#| label: profiles\n\n# Cartesian product → every possible combination of levels\nprofiles = pd.DataFrame(\n    list(product(brand_levels, ad_levels, price_levels)),\n    columns=[\"brand\", \"ad\", \"price\"]\n)\n\nm = len(profiles)  # total number of profiles\nprofiles.head()\n\n#| label: utils\n\n# Brand utilities\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\n\n# Ad utilities (penalty for ads ☹️)\na_util = {\"Yes\": -0.8, \"No\": 0.0}\n\n# Price utility function (each extra $1 costs 0.1 util)\nprice_util = lambda p: -0.1 * p\n\n#| label: design\n\nn_peeps = 100  # respondents\nn_tasks = 10   # choice tasks per respondent\nn_alts = 3     # alternatives shown per task\n\n#| label: simulate_one\n\ndef simulate_one(respondent_id: int) -&gt; pd.DataFrame:\n    \"\"\"Generate all choice tasks for a single respondent.\n\n    Parameters\n    ----------\n    respondent_id : int\n        Unique identifier (1..n_peeps)\n\n    Returns\n    -------\n    pandas.DataFrame\n        Wide table with one row per alternative × task.\n    \"\"\"\n    tasks = []  # collect each task’s alternatives\n\n    for task_no in range(1, n_tasks + 1):\n        # Randomly pick n_alts profiles (a richer design would force balance)\n        dat = profiles.sample(n=n_alts).copy()\n\n        # Add respondent & task identifiers *as first columns* (to match R order)\n        dat.insert(0, \"task\", task_no)\n        dat.insert(0, \"resp\", respondent_id)\n\n        # Deterministic utility (v)\n        dat[\"v\"] = (\n            dat[\"brand\"].map(b_util)\n            + dat[\"ad\"].map(a_util)\n            + price_util(dat[\"price\"])\n        )\n\n        # Stochastic (Gumbel Type‑I EV) noise (e)\n        e = -np.log(-np.log(np.random.uniform(size=n_alts)))\n        dat[\"u\"] = dat[\"v\"] + e  # total utility\n\n        # Choice indicator: 1 if max utility within the task\n        dat[\"choice\"] = (dat[\"u\"] == dat[\"u\"].max()).astype(int)\n\n        tasks.append(dat)\n\n    # Stack all tasks vertically\n    return pd.concat(tasks, ignore_index=True)\n\n#| label: simulate_all\n\n# Concatenate each respondent’s dataframes\nconjoint_data = pd.concat(\n    [simulate_one(i) for i in range(1, n_peeps + 1)],\n    ignore_index=True\n)\n\n# Keep only columns observable by the researcher (drop v, u, etc.)\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n\n# Quick peek 👀\nconjoint_data.head()\n\n\n\n\n\n\n\n\nresp\ntask\nbrand\nad\nprice\nchoice\n\n\n\n\n0\n1\n1\nP\nNo\n32\n0\n\n\n1\n1\n1\nN\nNo\n28\n0\n\n\n2\n1\n1\nN\nNo\n24\n1\n\n\n3\n1\n2\nH\nNo\n28\n0\n\n\n4\n1\n2\nH\nNo\n8\n1\n\n\n\n\n\n\n\nOutput from the simulation of the conjoint data:\n\n# Report dimensions (rows x cols) just like the R `cat()` statement\nrows, cols = conjoint_data.shape\nprint(f\"Dataset dimensions (rows x cols): {rows} x {cols}\")\n\n# Show first 10 rows without the implicit Pandas index (mirrors `row.names = FALSE`)\nprint(conjoint_data.head(10).to_string(index=False))\n\nDataset dimensions (rows x cols): 3000 x 6\n resp  task brand  ad  price  choice\n    1     1     P  No     32       0\n    1     1     N  No     28       0\n    1     1     N  No     24       1\n    1     2     H  No     28       0\n    1     2     H  No      8       1\n    1     2     H  No     32       0\n    1     3     N  No      8       1\n    1     3     H Yes     24       0\n    1     3     N Yes     16       0\n    1     4     N Yes      8       0\n\n\nThe simulated dataset contains r format(nrow(conjoint_data), big.mark = \",\") rows and r ncol(conjoint_data) columns (exactly 100 respondents × 10 tasks × 3 alternatives)."
  },
  {
    "objectID": "blog/Assignment 3/Assignment_3.html#preparing-the-data-for-estimation",
    "href": "blog/Assignment 3/Assignment_3.html#preparing-the-data-for-estimation",
    "title": "Multi-Nomial Logit (MNL) & Conjoint",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\ntodo: reshape and prep the data"
  },
  {
    "objectID": "blog/Assignment 3/Assignment_3.html#estimation-via-maximum-likelihood",
    "href": "blog/Assignment 3/Assignment_3.html#estimation-via-maximum-likelihood",
    "title": "Multi-Nomial Logit (MNL) & Conjoint",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\ntodo: Code up the log-likelihood function.\ntodo: Use optim() in R or scipy.optimize() in Python to find the MLEs for the 4 parameters (\\(\\beta_\\text{netflix}\\), \\(\\beta_\\text{prime}\\), \\(\\beta_\\text{ads}\\), \\(\\beta_\\text{price}\\)), as well as their standard errors (from the Hessian). For each parameter construct a 95% confidence interval."
  },
  {
    "objectID": "blog/Assignment 3/Assignment_3.html#estimation-via-bayesian-methods",
    "href": "blog/Assignment 3/Assignment_3.html#estimation-via-bayesian-methods",
    "title": "Multi-Nomial Logit (MNL) & Conjoint",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\ntodo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000.\nhint: Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta.\n_hint: instead of calculating post=lik*prior, you can work in the log-space and calculate log-post = log-lik + log-prior (this should enable you to re-use your log-likelihood function from the MLE section just above)_\nhint: King Markov (in the video) use a candidate distribution of a coin flip to decide whether to move left or right among his islands. Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous. So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal. Since this MVN has no covariances, you can sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005).\ntodo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution.\ntodo: report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to your results from the Maximum Likelihood approach."
  },
  {
    "objectID": "blog/Assignment 3/Assignment_3.html#discussion",
    "href": "blog/Assignment 3/Assignment_3.html#discussion",
    "title": "Multi-Nomial Logit (MNL) & Conjoint",
    "section": "6. Discussion",
    "text": "6. Discussion\ntodo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\ntodo: At a high level, discuss what change you would need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze “real world” conjoint data."
  }
]